{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6607339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/riadanas/Desktop/steven_bartlett_project/data/raw/DIARY_all_pod.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346ecf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269430, 18)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "# Keep a small sample for testing\n",
    "#df = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6289d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "video_ids = df['video_id'].unique()[5:10]\n",
    "df = df[df['video_id'].isin(video_ids)].sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ad723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 18)\n",
      "video_id\n",
      "Hik6OY-nk4c    28\n",
      "ldizQkuWpDE    27\n",
      "atejm2w2jWY    26\n",
      "It5_C6AF1pk    14\n",
      "0GQozcTPyO0     5\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!           28\n",
      "Body Language Expert Explains Why People Dislike You                                                  27\n",
      "Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!    26\n",
      "Exercise & Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!    14\n",
      "Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.       5\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33591</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>Hik6OY-nk4c</td>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your ...</td>\n",
       "      <td>Dr Jordan Peterson is a world-renowned former ...</td>\n",
       "      <td>2025-01-13T08:00:19Z</td>\n",
       "      <td>2046763</td>\n",
       "      <td>54184</td>\n",
       "      <td>6392</td>\n",
       "      <td>UgzCgbdzte3zTTQex6Z4AaABAg</td>\n",
       "      <td>This was very eye opening. I was sort of like ...</td>\n",
       "      <td>@JarreVonDuck</td>\n",
       "      <td>UCQMQlxaRQf04JUkceJ6obpw</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-23T21:52:03Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25333</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>0GQozcTPyO0</td>\n",
       "      <td>Hormone Expert: Control Your Hormones Control ...</td>\n",
       "      <td>Is your belly fat, stress, or burnout actually...</td>\n",
       "      <td>2025-03-27T06:00:00Z</td>\n",
       "      <td>932985</td>\n",
       "      <td>28358</td>\n",
       "      <td>2168</td>\n",
       "      <td>Ugzn7DFdwNefG9A-_bh4AaABAg.AGAV74jOz9WAGAdQyQHulW</td>\n",
       "      <td>You canâ€™t even spell hormone. Shush.</td>\n",
       "      <td>@Beauwagner</td>\n",
       "      <td>UCwppeGE5ffvJR3LWs2X1efQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-27T13:55:06Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Ugzn7DFdwNefG9A-_bh4AaABAg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             channel_name                channel_id     video_id  \\\n",
       "33591  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  Hik6OY-nk4c   \n",
       "25333  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  0GQozcTPyO0   \n",
       "\n",
       "                                             video_title  \\\n",
       "33591  Jordan B Peterson: You Need To Listen To Your ...   \n",
       "25333  Hormone Expert: Control Your Hormones Control ...   \n",
       "\n",
       "                                       video_description  \\\n",
       "33591  Dr Jordan Peterson is a world-renowned former ...   \n",
       "25333  Is your belly fat, stress, or burnout actually...   \n",
       "\n",
       "         video_published_at  view_count  video_like_count  comment_count  \\\n",
       "33591  2025-01-13T08:00:19Z     2046763             54184           6392   \n",
       "25333  2025-03-27T06:00:00Z      932985             28358           2168   \n",
       "\n",
       "                                              comment_id  \\\n",
       "33591                         UgzCgbdzte3zTTQex6Z4AaABAg   \n",
       "25333  Ugzn7DFdwNefG9A-_bh4AaABAg.AGAV74jOz9WAGAdQyQHulW   \n",
       "\n",
       "                                            comment_text         author  \\\n",
       "33591  This was very eye opening. I was sort of like ...  @JarreVonDuck   \n",
       "25333               You canâ€™t even spell hormone. Shush.    @Beauwagner   \n",
       "\n",
       "                      author_id  comment_like_count  comment_published_at  \\\n",
       "33591  UCQMQlxaRQf04JUkceJ6obpw                   0  2025-01-23T21:52:03Z   \n",
       "25333  UCwppeGE5ffvJR3LWs2X1efQ                   0  2025-03-27T13:55:06Z   \n",
       "\n",
       "       is_pinned  is_reply           parent_comment_id  \n",
       "33591      False     False                         NaN  \n",
       "25333      False      True  Ugzn7DFdwNefG9A-_bh4AaABAg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df['video_id'].value_counts())\n",
    "print(df['video_title'].value_counts())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e4da4",
   "metadata": {},
   "source": [
    "## Guest Name Processing - GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3447cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_guest_names_openrouter(description: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract true podcast guest names from a YouTube description using OpenRouter (Claude 3.5 / GPT-4-mini).\n",
    "    Ignores names used as references or examples.\n",
    "    \"\"\"\n",
    "    if not isinstance(description, str) or not description.strip():\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a podcast metadata assistant.\n",
    "\n",
    "    Task:\n",
    "    - Read the YouTube video description carefully.\n",
    "    - Identify ONLY the actual guest(s) who appear in the episode or are directly interviewed.\n",
    "    - If a guest's name was misspelled, correct it based on context.\n",
    "    - Make sure to not miss guests that go by nicknames (e.g., \"The Rock\" or \"MrBeast\").\n",
    "    - Ignore people mentioned just as examples, comparisons, or references (e.g., Warren Buffett, Elon Musk) unless they are clearly stated as guests.\n",
    "    - If multiple guests appear, include all of them.\n",
    "    - Preserve professional titles (e.g., \"Dr\", \"Prof\", \"Sir\") if present.\n",
    "    - Return a clean JSON list of guest names, for example:\n",
    "      [\"Morgan Housel\"]\n",
    "      or [\"Dr Andrew Huberman\", \"Lex Fridman\"]\n",
    "    - If no guest is clearly identified, return an empty list [].\n",
    "\n",
    "    Description:\n",
    "    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"openai/gpt-4o-mini\",  # you can change to \"openai/gpt-4o-mini\"\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "\n",
    "        content = completion.choices[0].message.content.strip()\n",
    "\n",
    "        # Try parsing JSON\n",
    "        try:\n",
    "            result = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r'\\[(.*?)\\]', content)\n",
    "            if match:\n",
    "                inner = match.group(1)\n",
    "                result = [n.strip().strip('\"').strip() for n in inner.split(\",\") if n.strip()]\n",
    "            else:\n",
    "                result = re.findall(r\"(?:Dr\\.?|Prof\\.?|Mr\\.?|Ms\\.?)?\\s?[A-Z][a-z]+(?:\\s[A-Z][a-z]+)+\", content)\n",
    "\n",
    "        if isinstance(result, str):\n",
    "            result = [result]\n",
    "        result = [r.strip() for r in result if r.strip()]\n",
    "        result = list(set(result))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing description: {e}\")\n",
    "        result = []\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ðŸ” Apply once per unique video_id\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def assign_guest_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply guest extraction once per unique video_id.\n",
    "    Adds a 'guest_list' column to the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create mapping: video_id â†’ guest list\n",
    "    mapping = {}\n",
    "    unique_videos = df.drop_duplicates(subset=\"video_id\")[[\"video_id\", \"video_description\"]]\n",
    "\n",
    "    for _, row in unique_videos.iterrows():\n",
    "        vid = row[\"video_id\"]\n",
    "        desc = row[\"video_description\"]\n",
    "        guests = get_guest_names_openrouter(desc)\n",
    "        mapping[vid] = guests\n",
    "\n",
    "    # Map results back to main DataFrame\n",
    "    df[\"guest_list\"] = df[\"video_id\"].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc3b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assign_guest_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b47e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guest_list\n",
       "[Dr Jordan Peterson]             28\n",
       "[Vanessa Van Edwards, Steven]    27\n",
       "[Master Shi Heng Yi]             26\n",
       "[Dr Stacy Sims]                  14\n",
       "[Dr. Sara Szal]                   5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['guest_list'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a58d885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>guest_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33591</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>Hik6OY-nk4c</td>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your ...</td>\n",
       "      <td>Dr Jordan Peterson is a world-renowned former ...</td>\n",
       "      <td>2025-01-13T08:00:19Z</td>\n",
       "      <td>2046763</td>\n",
       "      <td>54184</td>\n",
       "      <td>6392</td>\n",
       "      <td>UgzCgbdzte3zTTQex6Z4AaABAg</td>\n",
       "      <td>This was very eye opening. I was sort of like ...</td>\n",
       "      <td>@JarreVonDuck</td>\n",
       "      <td>UCQMQlxaRQf04JUkceJ6obpw</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-23T21:52:03Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25333</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>0GQozcTPyO0</td>\n",
       "      <td>Hormone Expert: Control Your Hormones Control ...</td>\n",
       "      <td>Is your belly fat, stress, or burnout actually...</td>\n",
       "      <td>2025-03-27T06:00:00Z</td>\n",
       "      <td>932985</td>\n",
       "      <td>28358</td>\n",
       "      <td>2168</td>\n",
       "      <td>Ugzn7DFdwNefG9A-_bh4AaABAg.AGAV74jOz9WAGAdQyQHulW</td>\n",
       "      <td>You canâ€™t even spell hormone. Shush.</td>\n",
       "      <td>@Beauwagner</td>\n",
       "      <td>UCwppeGE5ffvJR3LWs2X1efQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-27T13:55:06Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Ugzn7DFdwNefG9A-_bh4AaABAg</td>\n",
       "      <td>[Dr. Sara Szal]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             channel_name                channel_id     video_id  \\\n",
       "33591  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  Hik6OY-nk4c   \n",
       "25333  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  0GQozcTPyO0   \n",
       "\n",
       "                                             video_title  \\\n",
       "33591  Jordan B Peterson: You Need To Listen To Your ...   \n",
       "25333  Hormone Expert: Control Your Hormones Control ...   \n",
       "\n",
       "                                       video_description  \\\n",
       "33591  Dr Jordan Peterson is a world-renowned former ...   \n",
       "25333  Is your belly fat, stress, or burnout actually...   \n",
       "\n",
       "         video_published_at  view_count  video_like_count  comment_count  \\\n",
       "33591  2025-01-13T08:00:19Z     2046763             54184           6392   \n",
       "25333  2025-03-27T06:00:00Z      932985             28358           2168   \n",
       "\n",
       "                                              comment_id  \\\n",
       "33591                         UgzCgbdzte3zTTQex6Z4AaABAg   \n",
       "25333  Ugzn7DFdwNefG9A-_bh4AaABAg.AGAV74jOz9WAGAdQyQHulW   \n",
       "\n",
       "                                            comment_text         author  \\\n",
       "33591  This was very eye opening. I was sort of like ...  @JarreVonDuck   \n",
       "25333               You canâ€™t even spell hormone. Shush.    @Beauwagner   \n",
       "\n",
       "                      author_id  comment_like_count  comment_published_at  \\\n",
       "33591  UCQMQlxaRQf04JUkceJ6obpw                   0  2025-01-23T21:52:03Z   \n",
       "25333  UCwppeGE5ffvJR3LWs2X1efQ                   0  2025-03-27T13:55:06Z   \n",
       "\n",
       "       is_pinned  is_reply           parent_comment_id            guest_list  \n",
       "33591      False     False                         NaN  [Dr Jordan Peterson]  \n",
       "25333      False      True  Ugzn7DFdwNefG9A-_bh4AaABAg       [Dr. Sara Szal]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf69abbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr Jordan Peterson is a world-renowned former Professor of Psychology at the University of Toronto, and co-founder of the online education platform Peterson Academy. He is the author of bestselling books such as, â€˜12 Rules for Life: An Antidote to Chaosâ€™ and â€˜We Who Wrestle With God: Perceptions of the Divineâ€™.   00:00 Intro 02:30 The World Has Become Fractionated 05:23 Where Do We Find Ourselves Without Community? 08:41 How Do We Address Individualism in a Self-Centered Society? 15:21 Do Many P'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['video_description'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c769ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the heavy text column to speed up further LLM steps\n",
    "# df = df.drop(columns=[\"video_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27237f5",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab8a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Text Cleaning (light only)\n",
    "# ------------------------------------------------------\n",
    "def clean_comment(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning for comments:\n",
    "    - Remove @mentions\n",
    "    - Remove URLs\n",
    "    - Remove emojis / non-ascii\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    - Keep context words (no lemmatization, no stopword removal yet)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # remove mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    # remove emojis/non-ascii\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # lowercase + strip\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6cb8d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33591</th>\n",
       "      <td>This was very eye opening. I was sort of like this idiot that Jordan was trying to help.</td>\n",
       "      <td>this was very eye opening. i was sort of like this idiot that jordan was trying to help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25333</th>\n",
       "      <td>You canâ€™t even spell hormone. Shush.</td>\n",
       "      <td>you cant even spell hormone. shush.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26967</th>\n",
       "      <td>Iâ€™m 40 and I agree with this list it is working for meeeee I feel great!</td>\n",
       "      <td>im 40 and i agree with this list it is working for meeeee i feel great!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41914</th>\n",
       "      <td>Pure gold! Seeing a lot of confusion in the comments. You have to realise that all the advice is...</td>\n",
       "      <td>pure gold! seeing a lot of confusion in the comments. you have to realise that all the advice is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29542</th>\n",
       "      <td>i tried intermittent fasting and my period stopped coming.. iâ€™m still trying to fix itâ€¦ nobody w...</td>\n",
       "      <td>i tried intermittent fasting and my period stopped coming.. im still trying to fix it nobody war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29694</th>\n",
       "      <td>â€‹@@SevillaILoveit's not about disagreement though. Its literally saying or doing nothing when yo...</td>\n",
       "      <td>@'s not about disagreement though. its literally saying or doing nothing when you see others do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40205</th>\n",
       "      <td>Side note: What is it with spiritual people and tapping on tables? Is there an effect created by...</td>\n",
       "      <td>side note: what is it with spiritual people and tapping on tables? is there an effect created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39119</th>\n",
       "      <td>Steven, your questions are  amazingly on point. Wise, smart, thoughtful, well timed. Congratulat...</td>\n",
       "      <td>steven, your questions are  amazingly on point. wise, smart, thoughtful, well timed. congratulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46201</th>\n",
       "      <td>I prefer to be wary of first impressions and downplay their importance. How realistic are they? ...</td>\n",
       "      <td>i prefer to be wary of first impressions and downplay their importance. how realistic are they? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43841</th>\n",
       "      <td>Jokes on you when I check my phone I raise it in front of my face and don't scrunch up in defeat...</td>\n",
       "      <td>jokes on you when i check my phone i raise it in front of my face and don't scrunch up in defeat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              comment_text  \\\n",
       "33591             This was very eye opening. I was sort of like this idiot that Jordan was trying to help.   \n",
       "25333                                                                 You canâ€™t even spell hormone. Shush.   \n",
       "26967                             Iâ€™m 40 and I agree with this list it is working for meeeee I feel great!   \n",
       "41914  Pure gold! Seeing a lot of confusion in the comments. You have to realise that all the advice is...   \n",
       "29542  i tried intermittent fasting and my period stopped coming.. iâ€™m still trying to fix itâ€¦ nobody w...   \n",
       "29694  â€‹@@SevillaILoveit's not about disagreement though. Its literally saying or doing nothing when yo...   \n",
       "40205  Side note: What is it with spiritual people and tapping on tables? Is there an effect created by...   \n",
       "39119  Steven, your questions are  amazingly on point. Wise, smart, thoughtful, well timed. Congratulat...   \n",
       "46201  I prefer to be wary of first impressions and downplay their importance. How realistic are they? ...   \n",
       "43841  Jokes on you when I check my phone I raise it in front of my face and don't scrunch up in defeat...   \n",
       "\n",
       "                                                                                              cleaned_text  \n",
       "33591             this was very eye opening. i was sort of like this idiot that jordan was trying to help.  \n",
       "25333                                                                  you cant even spell hormone. shush.  \n",
       "26967                              im 40 and i agree with this list it is working for meeeee i feel great!  \n",
       "41914  pure gold! seeing a lot of confusion in the comments. you have to realise that all the advice is...  \n",
       "29542  i tried intermittent fasting and my period stopped coming.. im still trying to fix it nobody war...  \n",
       "29694  @'s not about disagreement though. its literally saying or doing nothing when you see others do ...  \n",
       "40205  side note: what is it with spiritual people and tapping on tables? is there an effect created by...  \n",
       "39119  steven, your questions are  amazingly on point. wise, smart, thoughtful, well timed. congratulat...  \n",
       "46201  i prefer to be wary of first impressions and downplay their importance. how realistic are they? ...  \n",
       "43841  jokes on you when i check my phone i raise it in front of my face and don't scrunch up in defeat...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply light cleaning to the comment_text column\n",
    "df[\"cleaned_text\"] = df[\"comment_text\"].apply(clean_comment)\n",
    "\n",
    "# Preview the results\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df[[\"comment_text\", \"cleaned_text\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f129b61",
   "metadata": {},
   "source": [
    "## Topic category (llama) -> Once per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04cbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "def get_topic_category(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Ollama to classify the video title into a topic category.\n",
    "    Example categories: health, mental health, productivity, finance, relationships, entrepreneurship, other.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Categorize the following YouTube video title into ONE broad category:\n",
    "    - health\n",
    "    - mental health / psychology\n",
    "    - productivity / personal development\n",
    "    - finance\n",
    "    - relationships\n",
    "    - entrepreneurship / business\n",
    "    - Religion / Spirituality\n",
    "    - Technology\n",
    "    - Education\n",
    "    - Lifestyle\n",
    "    - Entertainment\n",
    "    - other\n",
    "\n",
    "    Title: \"{title}\"\n",
    "\n",
    "    Return only the category name, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2:3b\",  # you can swap to another local model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a391d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Extract unique video_id/title pairs\n",
    "video_meta = df[[\"video_id\", \"video_title\"]].drop_duplicates()\n",
    "\n",
    "# Apply Ollama category classification\n",
    "video_meta[\"Topic_Category\"] = video_meta[\"video_title\"].apply(get_topic_category)\n",
    "\n",
    "# Merge back into main dataframe\n",
    "df = df.merge(video_meta[[\"video_id\", \"Topic_Category\"]], on=\"video_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f90aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic_Category\n",
       "health                        45\n",
       "relationship                  28\n",
       "mental health / psychology    27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd2b696",
   "metadata": {},
   "source": [
    "## Sentiment analysis (Gemma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb74efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### This pipeline takes YouTube comments â†’ cleans them â†’ detects sentiment, impact, guest mentions, and topic requests \n",
    "### using a hybrid NLP + LLM approach, balancing accuracy, cost, and scale.\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Helpers & resources\n",
    "# -----------------------------\n",
    "NEG_PHRASES = [\n",
    "    r\"\\bnot good\\b\", r\"\\bnot great\\b\", r\"\\bnot helpful\\b\", r\"\\bdon't like\\b\", r\"\\bdont like\\b\",\n",
    "    r\"\\bnot worth\\b\", r\"\\bwaste of time\\b\", r\"\\btoo long\\b\", r\"\\btoo slow\\b\", r\"\\btoo loud\\b\",\n",
    "    r\"\\bmisleading\\b\", r\"\\bclickbait\\b\", r\"\\bbiased\\b\", r\"\\bconfusing\\b\", r\"\\bannoying\\b\",\n",
    "    r\"\\bcringe\\b\", r\"\\bstupid\\b\", r\"\\bdumb\\b\", r\"\\bbad\\b\", r\"\\bawful\\b\", r\"\\bterrible\\b\",\n",
    "    r\"\\buseless\\b\", r\"\\bpointless\\b\", r\"\\bwrong\\b\", r\"\\bpoor (audio|sound|quality)\\b\",\n",
    "    r\"\\birrelevant\\b\", r\"\\boff\\-topic\\b\", r\"\\bproblem\\b\", r\"\\bissue\\b\", r\"\\bdisappoint(ing|ed)\\b\",\n",
    "    r\"\\brude\\b\", r\"\\boffensive\\b\", r\"\\bunfunny\\b\", r\"\\bboring\\b\", r\"\\blazy\\b\", r\"\\btoxic\\b\",\n",
    "    r\"\\bhate\\b\", r\"\\bgarbage\\b\", r\"\\bignorant\\b\", r\"\\bweird\\b\", r\"\\bnegative\\b\", r\"\\bbroken\\b\",\n",
    "    r\"\\bdownvote\\b\", r\"\\bterribly\\b\", r\"\\bdislike\\b\", r\"\\bpathetic\\b\", r\"\\bworse\\b\"\n",
    "]\n",
    "NEG_RE = re.compile(\"|\".join(NEG_PHRASES))\n",
    "\n",
    "SHORT_PRAISE_RE = re.compile(r\"^(nice|cool|great|good|amazing|awesome|wow|love|thanks|perfect)[.!]?$\", re.I)\n",
    "\n",
    "# Load spaCy (light model for entity recognition)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "def clean_comment(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def bucket_from_p(p: float) -> str:\n",
    "    if p < 0.35:\n",
    "        return \"Negative\"\n",
    "    if p > 0.65:\n",
    "        return \"Positive\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) HF Sentiment (RoBERTa)\n",
    "# -----------------------------\n",
    "_HF_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "_tok = AutoTokenizer.from_pretrained(_HF_MODEL)\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(_HF_MODEL)\n",
    "_model.eval()\n",
    "\n",
    "def roberta_probs(text: str) -> dict:\n",
    "    if not text:\n",
    "        return {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0}\n",
    "    with torch.no_grad():\n",
    "        inputs = _tok(text[:512], return_tensors=\"pt\")\n",
    "        logits = _model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return {\"neg\": float(probs[0]), \"neu\": float(probs[1]), \"pos\": float(probs[2])}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LLM Sentiment (Gemma 2B)\n",
    "# -----------------------------\n",
    "def gemma_score_01(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.5\n",
    "    prompt = f\"\"\"\n",
    "    You are a neutral linguistic expert analyzing sentiment.\n",
    "    Evaluate only the tone of the YouTube comment.\n",
    "    Consider sarcasm and negation carefully.\n",
    "    Return JSON only: {{\"score\": <float between -1.0 and 1.0>}}\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = ollama.chat(model=\"gemma:2b\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        j = json.loads(resp[\"message\"][\"content\"])\n",
    "        s = float(j.get(\"score\", 0.0))\n",
    "    except Exception:\n",
    "        s = 0.0\n",
    "\n",
    "    p = (s + 1.0) / 2.0\n",
    "    if NEG_RE.search(text) and p > 0.5:\n",
    "        p -= 0.25\n",
    "    if SHORT_PRAISE_RE.match(text) and p > 0.7:\n",
    "        p = 0.6\n",
    "    return float(np.clip(p, 0.0, 1.0))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Ensemble Sentiment\n",
    "# -----------------------------\n",
    "def ensemble_sentiment(text: str) -> dict:\n",
    "    text_clean = clean_comment(text.lower())\n",
    "    if not text_clean:\n",
    "        return {\"p_pos\": 0.5, \"bucket\": \"Neutral\", \"p_pos_llm\": 0.5, \"p_pos_hf\": 0.33, \"p_neg_hf\": 0.33}\n",
    "\n",
    "    p_pos_llm = gemma_score_01(text_clean)\n",
    "    hf = roberta_probs(text_clean)\n",
    "    p_pos_hf, p_neg_hf = hf[\"pos\"], hf[\"neg\"]\n",
    "\n",
    "    w_llm, w_hf = 0.4, 0.6\n",
    "    p_pos = w_llm * p_pos_llm + w_hf * p_pos_hf\n",
    "\n",
    "    if p_neg_hf - p_pos_hf > 0.20 and p_pos > 0.3:\n",
    "        p_pos -= 0.20\n",
    "\n",
    "    p_pos = float(np.clip(p_pos, 0.0, 1.0))\n",
    "    bucket = bucket_from_p(p_pos)\n",
    "    return {\n",
    "        \"p_pos\": p_pos,\n",
    "        \"bucket\": bucket,\n",
    "        \"p_pos_llm\": p_pos_llm,\n",
    "        \"p_pos_hf\": p_pos_hf,\n",
    "        \"p_neg_hf\": p_neg_hf,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Likes Weighting\n",
    "# -----------------------------\n",
    "def like_weight(likes: float) -> float:\n",
    "    if likes is None or likes <= 0:\n",
    "        return 1.0\n",
    "    if likes < 10:\n",
    "        return 1.2\n",
    "    if likes < 100:\n",
    "        return 2.0\n",
    "    if likes < 500:\n",
    "        return 3.0\n",
    "    return 4.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Improved Guest Extraction\n",
    "# -----------------------------\n",
    "\n",
    "BANNED_GUEST_WORDS = {\n",
    "    \"jesus\", \"praise jesus\", \"ohhhh\", \"video\", \"motivation\", \"johari\", \"topic\", \"content\"\n",
    "}\n",
    "\n",
    "GUEST_HINTS = re.compile(r\"(with|feat\\.|featuring|guest|bring back|have .* on|invite|episode with)\", re.I)\n",
    "TITLE_CUE_RE = re.compile(r\"feat\\.|featuring|with|guest|w/\", re.I)\n",
    "\n",
    "def extract_guests(text: str, video_title: str = \"\") -> list:\n",
    "    \"\"\"\n",
    "    Extract guest names from comment or title with 3-layer logic:\n",
    "    1. spaCy PERSON entities (fast)\n",
    "    2. Gemma (if hints present)\n",
    "    3. Fallback to video title if contains 'feat.' etc.\n",
    "    \"\"\"\n",
    "    text = clean_comment(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    names = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "    # If found via spaCy, trust them (light clean)\n",
    "    if names:\n",
    "        names = [n for n in names if len(n) > 2 and n.lower() not in BANNED_GUEST_WORDS]\n",
    "        return list(set(names))\n",
    "\n",
    "    # Only call LLM if hints present\n",
    "    if not GUEST_HINTS.search(text):\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing a YouTube comment on a podcast episode.\n",
    "\n",
    "    Your goal is to extract only the names of people the viewer *explicitly or implicitly wants to appear as guests* on future episodes.\n",
    "\n",
    "    Focus on:\n",
    "    - Mentions framed as requests or desires (e.g. â€œbring X onâ€, â€œinvite Yâ€, â€œyou should talk to Zâ€, â€œhave A backâ€).\n",
    "    - Include both full names and recognizable single names or titles (e.g. â€œDr. Hubermanâ€, â€œPetersonâ€, â€œElon Muskâ€).\n",
    "    - Ignore names mentioned for other reasons (e.g. as examples, comparisons, or in stories).\n",
    "    - Exclude religious figures, fictional characters, vague references, or non-human entities.\n",
    "\n",
    "    Return strictly a **JSON array of names** (no text explanation, no duplicates, no empty entries).\n",
    "\n",
    "    Example:\n",
    "    [\"Jordan Peterson\", \"Dr. Rhonda Patrick\", \"Elon Musk\"]\n",
    "\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = ollama.chat(model=\"gemma:2b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        names = json.loads(resp[\"message\"][\"content\"])\n",
    "    except Exception:\n",
    "        names = []\n",
    "\n",
    "    # Post-filter\n",
    "    if isinstance(names, list):\n",
    "        names = [\n",
    "            n.strip() for n in names\n",
    "            if n and len(n) > 2 and n[0].isupper() and n.lower() not in BANNED_GUEST_WORDS\n",
    "        ]\n",
    "    else:\n",
    "        names = []\n",
    "\n",
    "    # Fallback from video title\n",
    "    if not names and TITLE_CUE_RE.search(video_title):\n",
    "        doc_t = nlp(video_title)\n",
    "        title_names = [ent.text.strip() for ent in doc_t.ents if ent.label_ == \"PERSON\"]\n",
    "        names.extend(title_names)\n",
    "\n",
    "    return list(set(names))\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Topic Request Extraction\n",
    "# -----------------------------\n",
    "TOPIC_HINTS = re.compile(r\"(talk about|episode on|discuss|cover|would love|should do|next guest|topic|content about)\", re.I)\n",
    "\n",
    "def extract_topics(text: str) -> list:\n",
    "    text = clean_comment(text)\n",
    "    if not text or not TOPIC_HINTS.search(text):\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing a YouTube comment on a podcast episode.\n",
    "\n",
    "    Your goal is to extract *specific topics or subjects the viewer explicitly wants discussed in future episodes.*\n",
    "\n",
    "    Focus on:\n",
    "    - Requests or suggestions like â€œtalk about Xâ€, â€œdo an episode on Yâ€, â€œdiscuss more about Zâ€, â€œIâ€™d love a conversation onâ€¦â€ etc.\n",
    "    - Extract concise, meaningful topics (1â€“4 words) representing what the viewer wants to learn or hear more about.\n",
    "    - Ignore vague mentions, compliments, or generic categories (like â€œscienceâ€, â€œhealthâ€, â€œAIâ€) unless clearly framed as requests.\n",
    "    - Exclude random nouns or things already being discussed.\n",
    "\n",
    "    Return strictly a **JSON list of short topic phrases** (no explanation, no duplicates).\n",
    "\n",
    "    Example:\n",
    "    [\"mental health in men\", \"AI and creativity\", \"nutrition and longevity\"]\n",
    "\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = ollama.chat(model=\"gemma:2b\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        topics = json.loads(resp[\"message\"][\"content\"])\n",
    "        if isinstance(topics, list):\n",
    "            return topics\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Apply All to DataFrame\n",
    "# -----------------------------\n",
    "def apply_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"cleaned_text\"] = df[\"cleaned_text\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Sentiment\n",
    "    res = df[\"cleaned_text\"].progress_apply(ensemble_sentiment)\n",
    "    df[\"sentiment_p_pos\"] = res.apply(lambda r: r[\"p_pos\"])\n",
    "    df[\"sentiment_bucket\"] = res.apply(lambda r: r[\"bucket\"])\n",
    "    df[\"p_pos_llm\"] = res.apply(lambda r: r[\"p_pos_llm\"])\n",
    "    df[\"p_pos_hf\"] = res.apply(lambda r: r[\"p_pos_hf\"])\n",
    "    df[\"p_neg_hf\"] = res.apply(lambda r: r[\"p_neg_hf\"])\n",
    "\n",
    "    # Weights\n",
    "    df[\"comment_like_count\"] = df[\"comment_like_count\"].fillna(0).astype(int)\n",
    "    df[\"comment_weight\"] = df[\"comment_like_count\"].apply(like_weight)\n",
    "    df[\"impact_weighted_sentiment\"] = df[\"sentiment_p_pos\"] * df[\"comment_weight\"]\n",
    "\n",
    "    # New columns â€” guests & topics\n",
    "    df[\"guest_mentions\"] = df.progress_apply(\n",
    "        lambda r: extract_guests(r[\"cleaned_text\"], r.get(\"video_title\", \"\")), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"topic_requests\"] = df[\"cleaned_text\"].progress_apply(extract_topics)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac05a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:44<00:00,  2.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00,  6.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 190.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Run\n",
    "# -----------------------------\n",
    "df_sent = apply_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "500064be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment_p_pos</th>\n",
       "      <th>sentiment_bucket</th>\n",
       "      <th>p_pos_llm</th>\n",
       "      <th>p_pos_hf</th>\n",
       "      <th>p_neg_hf</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_weight</th>\n",
       "      <th>impact_weighted_sentiment</th>\n",
       "      <th>guest_list</th>\n",
       "      <th>topic_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was very eye opening. I was sort of like this idiot that Jordan was trying to help.</td>\n",
       "      <td>this was very eye opening. i was sort of like this idiot that jordan was trying to help.</td>\n",
       "      <td>0.527825</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.268005</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.527825</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You canâ€™t even spell hormone. Shush.</td>\n",
       "      <td>you cant even spell hormone. shush.</td>\n",
       "      <td>0.206989</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.878349</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206989</td>\n",
       "      <td>[Dr. Sara Szal]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iâ€™m 40 and I agree with this list it is working for meeeee I feel great!</td>\n",
       "      <td>im 40 and i agree with this list it is working for meeeee i feel great!</td>\n",
       "      <td>0.881234</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.985389</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.057480</td>\n",
       "      <td>[Dr Stacy Sims]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pure gold! Seeing a lot of confusion in the comments. You have to realise that all the advice is...</td>\n",
       "      <td>pure gold! seeing a lot of confusion in the comments. you have to realise that all the advice is...</td>\n",
       "      <td>0.287034</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.128390</td>\n",
       "      <td>0.426005</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.344441</td>\n",
       "      <td>[Vanessa Van Edwards, Steven]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i tried intermittent fasting and my period stopped coming.. iâ€™m still trying to fix itâ€¦ nobody w...</td>\n",
       "      <td>i tried intermittent fasting and my period stopped coming.. im still trying to fix it nobody war...</td>\n",
       "      <td>0.253586</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.897784</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253586</td>\n",
       "      <td>[Dr Stacy Sims]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          comment_text  \\\n",
       "0             This was very eye opening. I was sort of like this idiot that Jordan was trying to help.   \n",
       "1                                                                 You canâ€™t even spell hormone. Shush.   \n",
       "2                             Iâ€™m 40 and I agree with this list it is working for meeeee I feel great!   \n",
       "3  Pure gold! Seeing a lot of confusion in the comments. You have to realise that all the advice is...   \n",
       "4  i tried intermittent fasting and my period stopped coming.. iâ€™m still trying to fix itâ€¦ nobody w...   \n",
       "\n",
       "                                                                                          cleaned_text  \\\n",
       "0             this was very eye opening. i was sort of like this idiot that jordan was trying to help.   \n",
       "1                                                                  you cant even spell hormone. shush.   \n",
       "2                              im 40 and i agree with this list it is working for meeeee i feel great!   \n",
       "3  pure gold! seeing a lot of confusion in the comments. you have to realise that all the advice is...   \n",
       "4  i tried intermittent fasting and my period stopped coming.. im still trying to fix it nobody war...   \n",
       "\n",
       "   sentiment_p_pos sentiment_bucket  p_pos_llm  p_pos_hf  p_neg_hf  \\\n",
       "0         0.527825          Neutral      0.820  0.333042  0.268005   \n",
       "1         0.206989         Negative      0.500  0.011648  0.878349   \n",
       "2         0.881234         Positive      0.725  0.985389  0.003019   \n",
       "3         0.287034         Negative      0.525  0.128390  0.426005   \n",
       "4         0.253586         Negative      0.625  0.005976  0.897784   \n",
       "\n",
       "   comment_like_count  comment_weight  impact_weighted_sentiment  \\\n",
       "0                   0             1.0                   0.527825   \n",
       "1                   0             1.0                   0.206989   \n",
       "2                   2             1.2                   1.057480   \n",
       "3                   3             1.2                   0.344441   \n",
       "4                   0             1.0                   0.253586   \n",
       "\n",
       "                      guest_list topic_requests  \n",
       "0           [Dr Jordan Peterson]             []  \n",
       "1                [Dr. Sara Szal]             []  \n",
       "2                [Dr Stacy Sims]             []  \n",
       "3  [Vanessa Van Edwards, Steven]             []  \n",
       "4                [Dr Stacy Sims]             []  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent[[\n",
    "    \"comment_text\",\n",
    "    \"cleaned_text\",\n",
    "    \"sentiment_p_pos\",\n",
    "    \"sentiment_bucket\",\n",
    "    \"p_pos_llm\",\n",
    "    \"p_pos_hf\",\n",
    "    \"p_neg_hf\",\n",
    "    \"comment_like_count\",\n",
    "    \"comment_weight\",\n",
    "    \"impact_weighted_sentiment\",\n",
    "    \"guest_list\",\n",
    "    \"topic_requests\"\n",
    "]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76d6f3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_bucket\n",
       "Negative    46\n",
       "Positive    33\n",
       "Neutral     21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['sentiment_bucket'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec65355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guest_mentions\n",
       "[]                                                  80\n",
       "[Elon Musk, Jordan Peterson, Dr. Rhonda Patrick]     5\n",
       "[Elon Musk, Dr. Rhonda Patrick]                      5\n",
       "[jordan]                                             4\n",
       "[jordan peterson]                                    2\n",
       "[shi heng]                                           1\n",
       "[babysitter]                                         1\n",
       "[ken robinson]                                       1\n",
       "[steve, shi]                                         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['guest_mentions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3946ac6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_requests\n",
       "[]                                99\n",
       "[relationship crisis in china]     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['topic_requests'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41fdd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sent.to_csv('/Users/riadanas/Desktop/MLE Diary of a CEO/data/processed/processed_snapshot4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6a907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "972a73af",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (Boosted logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9728684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Fast, Scalable Comment NLP (with improved, intent-driven guest/topic extraction)\n",
    "# ===========================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "import ollama\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) GLOBALS & RESOURCES\n",
    "# -----------------------------\n",
    "tqdm.pandas()\n",
    "\n",
    "NEG_PHRASES = [\n",
    "    r\"\\bnot good\\b\", r\"\\bnot great\\b\", r\"\\bnot helpful\\b\", r\"\\bdon't like\\b\", r\"\\bdont like\\b\",\n",
    "    r\"\\bnot worth\\b\", r\"\\bwaste of time\\b\", r\"\\btoo long\\b\", r\"\\btoo slow\\b\", r\"\\btoo loud\\b\",\n",
    "    r\"\\bmisleading\\b\", r\"\\bclickbait\\b\", r\"\\bbiased\\b\", r\"\\bconfusing\\b\", r\"\\bannoying\\b\",\n",
    "    r\"\\bcringe\\b\", r\"\\bstupid\\b\", r\"\\bdumb\\b\", r\"\\bbad\\b\", r\"\\bawful\\b\", r\"\\bterrible\\b\",\n",
    "    r\"\\buseless\\b\", r\"\\bpointless\\b\", r\"\\bwrong\\b\", r\"\\bpoor (audio|sound|quality)\\b\",\n",
    "    r\"\\birrelevant\\b\", r\"\\boff\\-topic\\b\", r\"\\bproblem\\b\", r\"\\bissue\\b\", r\"\\bdisappoint(ing|ed)\\b\",\n",
    "    r\"\\brude\\b\", r\"\\boffensive\\b\", r\"\\bunfunny\\b\", r\"\\bboring\\b\", r\"\\blazy\\b\", r\"\\btoxic\\b\",\n",
    "    r\"\\bhate\\b\", r\"\\bgarbage\\b\", r\"\\bignorant\\b\", r\"\\bweird\\b\", r\"\\bnegative\\b\", r\"\\bbroken\\b\",\n",
    "    r\"\\bdownvote\\b\", r\"\\bterribly\\b\", r\"\\bdislike\\b\", r\"\\bpathetic\\b\", r\"\\bworse\\b\"\n",
    "]\n",
    "NEG_RE = re.compile(\"|\".join(NEG_PHRASES), re.I)\n",
    "SHORT_PRAISE_RE = re.compile(r\"^(nice|cool|great|good|amazing|awesome|wow|love|thanks|perfect)[.!]?$\", re.I)\n",
    "\n",
    "# Sentiment LLM pre-filter (nudges LLM use for nuanced tone)\n",
    "LLM_HINTS = re.compile(r\"(sarcasm|/s|lol|lmao|haha|not\\s+\\w+|but|however|although|though|guess|apparently)\", re.I)\n",
    "\n",
    "# ---------- INTENT TEMPLATES (NEW) ----------\n",
    "# Guest intent phrases (bring/invite/have on/back/etc.)\n",
    "GUEST_INTENT = re.compile(\n",
    "    r\"(bring\\s+(?:him|her|them|[A-Z][\\w'.\\-]+)\\s+(?:on|back)|\"\n",
    "    r\"invite\\s+(?:him|her|them|[A-Z][\\w'.\\-]+)|\"\n",
    "    r\"have\\s+(?:him|her|them|[A-Z][\\w'.\\-]+)\\s+on|\"\n",
    "    r\"get\\s+(?:him|her|them|[A-Z][\\w'.\\-]+)\\s+on|\"\n",
    "    r\"episode\\s+with\\s+[A-Z][\\w'.\\-]+|\"\n",
    "    r\"talk\\s+to\\s+[A-Z][\\w'.\\-]+|\"\n",
    "    r\"next\\s+guest|feature\\s+[A-Z][\\w'.\\-]+)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# Negative/anti-intent for guest requests (e.g., \"don't bring X\")\n",
    "NEG_INTENT = re.compile(r\"(don'?t|do not|please\\s+don'?t|no\\s+more|stop)\\s+(?:bring|get|have|invite|feature|interview)\", re.I)\n",
    "\n",
    "# Topic intent phrases (talk/discuss/more on/do one about/cover/etc.)\n",
    "TOPIC_INTENT = re.compile(\n",
    "    r\"(talk\\s+about|discuss|more\\s+on|more\\s+of|do\\s+one\\s+about|episode\\s+on|video\\s+on|\"\n",
    "    r\"cover|speak\\s+about|conversation\\s+on|should\\s+talk\\s+about|\"\n",
    "    r\"(?:i(?:'d)?\\s+)?(?:love|want)\\s+(?:to\\s+hear|an?\\s+episode)\\s+on|\"\n",
    "    r\"deep\\s+dive\\s+on|more\\s+content\\s+on)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# Capture after trigger up to punctuation/EOL\n",
    "TOPIC_CAPTURE = re.compile(\n",
    "    r\"(?:talk\\s+about|discuss|more\\s+on|more\\s+of|do\\s+one\\s+about|episode\\s+on|video\\s+on|\"\n",
    "    r\"cover|speak\\s+about|conversation\\s+on|should\\s+talk\\s+about|\"\n",
    "    r\"(?:i(?:'d)?\\s+)?(?:love|want)\\s+(?:to\\s+hear|an?\\s+episode)\\s+on|\"\n",
    "    r\"deep\\s+dive\\s+on|more\\s+content\\s+on)\\s+([^\\.!\\?\\n]+)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# spaCy NER (parser enabled because we use noun_chunks)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"lemmatizer\"])\n",
    "\n",
    "# HF model\n",
    "_HF_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "_tok = AutoTokenizer.from_pretrained(_HF_MODEL)\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(_HF_MODEL)\n",
    "_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "_model.to(DEVICE)\n",
    "\n",
    "CACHE_PATH = os.path.abspath(\"./sentiment_cache.sqlite\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) UTILS\n",
    "# -----------------------------\n",
    "def clean_comment(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text.strip()\n",
    "\n",
    "def bucket_from_p(p: float) -> str:\n",
    "    if p < 0.35: return \"Negative\"\n",
    "    if p > 0.65: return \"Positive\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def like_weight(likes: float) -> float:\n",
    "    if likes is None or likes <= 0: return 1.0\n",
    "    if likes < 10: return 1.2\n",
    "    if likes < 100: return 2.0\n",
    "    if likes < 500: return 3.0\n",
    "    return 4.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) CACHE (sentiment + guests/topics)\n",
    "# -----------------------------\n",
    "def _ensure_cache():\n",
    "    con = sqlite3.connect(CACHE_PATH)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS sentiments (\n",
    "        comment_id TEXT PRIMARY KEY,\n",
    "        text_hash  TEXT NOT NULL,\n",
    "        p_pos      REAL NOT NULL,\n",
    "        bucket     TEXT NOT NULL,\n",
    "        p_pos_llm  REAL NOT NULL,\n",
    "        p_pos_hf   REAL NOT NULL,\n",
    "        p_neg_hf   REAL NOT NULL,\n",
    "        created_at TEXT NOT NULL\n",
    "      );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS meta_extractions (\n",
    "        comment_id  TEXT PRIMARY KEY,\n",
    "        text_hash   TEXT NOT NULL,\n",
    "        guest_json  TEXT NOT NULL,\n",
    "        topic_json  TEXT NOT NULL,\n",
    "        created_at  TEXT NOT NULL\n",
    "      );\n",
    "    \"\"\")\n",
    "    con.commit(); con.close()\n",
    "\n",
    "def cache_get_sentiments(comment_ids, text_hashes):\n",
    "    if not comment_ids: return {}\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    qmarks = \",\".join([\"?\"] * len(comment_ids))\n",
    "    rows = cur.execute(f\"SELECT * FROM sentiments WHERE comment_id IN ({qmarks})\", comment_ids).fetchall()\n",
    "    con.close()\n",
    "    out = {}\n",
    "    for (cid, th, p_pos, bucket, p_pos_llm, p_pos_hf, p_neg_hf, created_at) in rows:\n",
    "        if text_hashes.get(cid) == th:\n",
    "            out[cid] = dict(p_pos=p_pos, bucket=bucket, p_pos_llm=p_pos_llm, p_pos_hf=p_pos_hf, p_neg_hf=p_neg_hf)\n",
    "    return out\n",
    "\n",
    "def cache_put_sentiments(rows):\n",
    "    if not rows: return\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "      INSERT INTO sentiments (comment_id, text_hash, p_pos, bucket, p_pos_llm, p_pos_hf, p_neg_hf, created_at)\n",
    "      VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "      ON CONFLICT(comment_id) DO UPDATE SET\n",
    "        text_hash=excluded.text_hash,\n",
    "        p_pos=excluded.p_pos,\n",
    "        bucket=excluded.bucket,\n",
    "        p_pos_llm=excluded.p_pos_llm,\n",
    "        p_pos_hf=excluded.p_pos_hf,\n",
    "        p_neg_hf=excluded.p_neg_hf,\n",
    "        created_at=excluded.created_at;\n",
    "    \"\"\", [(cid, th, ppos, buck, pll, phf, nhf, datetime.utcnow().isoformat()) for (cid, th, ppos, buck, pll, phf, nhf) in rows])\n",
    "    con.commit(); con.close()\n",
    "\n",
    "def cache_get_meta(comment_ids, text_hashes):\n",
    "    if not comment_ids: return {}\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    qmarks = \",\".join([\"?\"] * len(comment_ids))\n",
    "    rows = cur.execute(f\"SELECT * FROM meta_extractions WHERE comment_id IN ({qmarks})\", comment_ids).fetchall()\n",
    "    con.close()\n",
    "    out = {}\n",
    "    for (cid, th, gj, tj, created_at) in rows:\n",
    "        if text_hashes.get(cid) == th:\n",
    "            out[cid] = {\"guest_mentions\": json.loads(gj), \"topic_requests\": json.loads(tj)}\n",
    "    return out\n",
    "\n",
    "def cache_put_meta(rows):\n",
    "    if not rows: return\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "      INSERT INTO meta_extractions (comment_id, text_hash, guest_json, topic_json, created_at)\n",
    "      VALUES (?, ?, ?, ?, ?)\n",
    "      ON CONFLICT(comment_id) DO UPDATE SET\n",
    "        text_hash=excluded.text_hash,\n",
    "        guest_json=excluded.guest_json,\n",
    "        topic_json=excluded.topic_json,\n",
    "        created_at=excluded.created_at;\n",
    "    \"\"\", [(cid, th, json.dumps(guests), json.dumps(topics), datetime.utcnow().isoformat()) for (cid, th, guests, topics) in rows])\n",
    "    con.commit(); con.close()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) HF SENTIMENT (BATCHED)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def roberta_probs_batch(texts: List[str], batch_size: int = 512, max_len: int = 128) -> np.ndarray:\n",
    "    probs_all = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        enc = _tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        logits = _model(**enc).logits\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "    return np.vstack(probs_all) if probs_all else np.zeros((0, 3), dtype=float)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) LLM SENTIMENT (PARALLEL)\n",
    "# -----------------------------\n",
    "def gemma_score_01(text: str, retries: int = 2, model_name: str = \"gemma:2b\") -> float:\n",
    "    if not text: return 0.5\n",
    "    prompt = f\"\"\"\n",
    "    You are a neutral linguistic expert analyzing sentiment only.\n",
    "    Consider sarcasm and negation carefully.\n",
    "    Return JSON only: {{\"score\": <float between -1.0 and 1.0>}}\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\".strip()\n",
    "    for _ in range(max(1, retries)):\n",
    "        try:\n",
    "            resp = ollama.chat(model=model_name, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "            j = json.loads(resp[\"message\"][\"content\"])\n",
    "            s = float(j.get(\"score\", 0.0))\n",
    "            p = (s + 1.0) / 2.0\n",
    "            if NEG_RE.search(text) and p > 0.5: p -= 0.25\n",
    "            if SHORT_PRAISE_RE.match(text) and p > 0.7: p = 0.6\n",
    "            return float(np.clip(p, 0.0, 1.0))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return 0.5\n",
    "\n",
    "def parallel_llm_scores(texts: List[str], max_workers: int = 8, model_name: str = \"gemma:2b\") -> List[float]:\n",
    "    scores = [0.5] * len(texts)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(gemma_score_01, t, 2, model_name): i for i, t in enumerate(texts)}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"LLM (parallel)\"):\n",
    "            i = futs[fut]\n",
    "            try: scores[i] = fut.result()\n",
    "            except Exception: scores[i] = 0.5\n",
    "    return scores\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) ENSEMBLE SENTIMENT\n",
    "# -----------------------------\n",
    "def ensemble_scores_fast(texts: List[str],\n",
    "                         call_llm_if_uncertain: bool = True,\n",
    "                         llm_max_workers: int = 8,\n",
    "                         llm_model: str = \"gemma:2b\") -> Dict[str, np.ndarray]:\n",
    "    cleaned = [clean_comment(t.lower()) for t in texts]\n",
    "    hf_probs = roberta_probs_batch(cleaned, batch_size=512, max_len=128)\n",
    "    if hf_probs.shape[0] == 0:\n",
    "        n = len(texts)\n",
    "        return dict(p_pos=np.full(n,0.5), p_pos_hf=np.full(n,1/3), p_neg_hf=np.full(n,1/3), p_pos_llm=np.full(n,0.5))\n",
    "\n",
    "    p_neg_hf, p_neu_hf, p_pos_hf = hf_probs[:,0], hf_probs[:,1], hf_probs[:,2]\n",
    "\n",
    "    if call_llm_if_uncertain:\n",
    "        uncertain = np.abs(p_pos_hf - p_neg_hf) < 0.20\n",
    "        hints = np.array([bool(LLM_HINTS.search(t)) for t in cleaned])\n",
    "        run_llm_mask = np.logical_or(uncertain, hints)\n",
    "    else:\n",
    "        run_llm_mask = np.zeros(len(texts), dtype=bool)\n",
    "\n",
    "    p_pos_llm = p_pos_hf.copy()\n",
    "    if run_llm_mask.any():\n",
    "        idxs = np.where(run_llm_mask)[0].tolist()\n",
    "        subset = [cleaned[i] for i in idxs]\n",
    "        llm_scores = parallel_llm_scores(subset, max_workers=llm_max_workers, model_name=llm_model)\n",
    "        for loc, val in zip(idxs, llm_scores):\n",
    "            p_pos_llm[loc] = val\n",
    "\n",
    "    w_llm, w_hf = 0.4, 0.6\n",
    "    p_pos = w_llm * p_pos_llm + w_hf * p_pos_hf\n",
    "    adjust_mask = (p_neg_hf - p_pos_hf) > 0.20\n",
    "    p_pos[adjust_mask] = np.maximum(0.0, p_pos[adjust_mask] - 0.20)\n",
    "    p_pos = np.clip(p_pos, 0.0, 1.0)\n",
    "\n",
    "    return dict(p_pos=p_pos, p_pos_hf=p_pos_hf, p_neg_hf=p_neg_hf, p_pos_llm=p_pos_llm)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6) >>> INTENT-DRIVEN GUEST EXTRACTION (precision-focused, batched) <<<\n",
    "# =====================================================================\n",
    "\n",
    "# Junk terms that often slip in\n",
    "BAD_PERSON_LIKE = {\n",
    "    \"community note\",\"intro\",\"idk\",\"drs\",\"omad\",\"keto\",\"dang\",\"app\",\"stfu\",\n",
    "    \"nicotine\",\"propylene glycol\",\"schizophrenia\",\"hunter gatherers\",\"good luck\",\"pure gold\",\n",
    "    \"someone\",\"somebody\",\"everyone\",\"anyone\",\"anybody\",\"people\",\"guest\",\"guests\",\"host\",\n",
    "    \"video\",\"content\",\"topic\",\"episode\",\"pls\",\"please\",\"thanks\",\"thank you\"\n",
    "}\n",
    "TITLE_CUE_RE = re.compile(r\"feat\\.|featuring|with|guest|w/\", re.I)\n",
    "\n",
    "# Optional known roster to whitelist single surnames (fill from your dataset if you want)\n",
    "KNOWN_GUESTS = set()  # e.g., {\"Jordan Peterson\",\"Ben Bikman\",\"Georgia Ede\", ...}\n",
    "KNOWN_SURNAMES = {n.split()[-1] for n in KNOWN_GUESTS}\n",
    "\n",
    "# Optional alias map for common misspellings / short forms\n",
    "NAME_ALIASES = {\n",
    "    \"daniel priestly\": \"Daniel Priestley\",\n",
    "    \"ben bikman\": \"Benjamin Bikman\",\n",
    "    \"dr bikman\": \"Benjamin Bikman\",\n",
    "    \"dr. bikman\": \"Benjamin Bikman\",\n",
    "    \"jp\": \"Jordan Peterson\",\n",
    "    # Enable only if OK to default lone surname to JP for your dataset:\n",
    "    # \"peterson\": \"Jordan Peterson\",\n",
    "    \"dr k\": \"Dr K\",\n",
    "}\n",
    "\n",
    "def _norm_name(s: str) -> Optional[str]:\n",
    "    \"\"\"Normalize a candidate name; map aliases; keep title-cased.\"\"\"\n",
    "    if not s: return None\n",
    "    s2 = re.sub(r\"[^A-Za-z'\\-\\.\\s]\", \" \", s).strip()\n",
    "    s2 = re.sub(r\"\\s+\", \" \", s2)\n",
    "    if not s2: return None\n",
    "    low = s2.lower()\n",
    "    if low in BAD_PERSON_LIKE: return None\n",
    "    if low in NAME_ALIASES:\n",
    "        return NAME_ALIASES[low]\n",
    "    toks = s2.split()\n",
    "    def fix(tok):\n",
    "        return tok if tok.lower().startswith((\"dr\", \"prof\", \"sir\", \"mr\", \"ms\", \"mrs\")) else tok.title()\n",
    "    s3 = \" \".join(fix(t) for t in toks)\n",
    "    if len(toks) == 1:\n",
    "        if toks[0] in KNOWN_SURNAMES:\n",
    "            return toks[0].title()\n",
    "        return None\n",
    "    return s3\n",
    "\n",
    "def _dedupe_names(names: List[str]) -> List[str]:\n",
    "    \"\"\"Prefer longer forms (e.g., 'Jordan Peterson' over 'Dr Peterson').\"\"\"\n",
    "    if not names: return []\n",
    "    names_sorted = sorted(set(names), key=lambda n: (-len(n.split()), n))\n",
    "    result, seen_keys = [], set()\n",
    "    for n in names_sorted:\n",
    "        key = n.lower().replace(\"dr \", \"\").replace(\"prof \", \"\")\n",
    "        surname = n.split()[-1].lower()\n",
    "        if (key in seen_keys) or any(surname == r.split()[-1].lower() for r in result):\n",
    "            continue\n",
    "        result.append(n)\n",
    "        seen_keys.add(key)\n",
    "    return result\n",
    "\n",
    "def _intent_spans(text: str) -> List[Tuple[int,int]]:\n",
    "    return [m.span() for m in GUEST_INTENT.finditer(text or \"\")]\n",
    "\n",
    "def _persons_near_spans(doc, spans: List[Tuple[int,int]], window: int = 50) -> List[str]:\n",
    "    out = []\n",
    "    for (s, e) in spans:\n",
    "        left, right = max(0, s - window), min(len(doc.text), e + window)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\" and ent.start_char >= left and ent.end_char <= right:\n",
    "                n = _norm_name(ent.text)\n",
    "                if n: out.append(n)\n",
    "    return out\n",
    "\n",
    "def extract_guests_batch(texts: List[str], titles: Optional[List[str]] = None,\n",
    "                         max_workers: int = 8, llm_model: str = \"gemma:2b\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Faster & stricter:\n",
    "      â€¢ Must have guest intent.\n",
    "      â€¢ Use spaCy PERSON near the intent phrase (proximity filter).\n",
    "      â€¢ Skip negative-intent (e.g. \"don't bring X\").\n",
    "      â€¢ LLM fallback only if spaCy near-intent returns nothing.\n",
    "      â€¢ Optional: title fallback when 'feat./with' in title.\n",
    "    \"\"\"\n",
    "    titles = titles or [\"\"] * len(texts)\n",
    "    cleaned = [clean_comment(t) for t in texts]\n",
    "    results = [[] for _ in cleaned]\n",
    "\n",
    "    # 0) quickly mark rows with intent and non-negative intent\n",
    "    intent_mask = [bool(GUEST_INTENT.search(t or \"\")) for t in cleaned]\n",
    "    negative_mask = [bool(NEG_INTENT.search(t or \"\")) for t in cleaned]\n",
    "\n",
    "    # 1) spaCy in batch only for intent rows (speeds things up)\n",
    "    to_proc_idx = [i for i, ok in enumerate(intent_mask) if ok and not negative_mask[i]]\n",
    "    to_proc_texts = [cleaned[i] for i in to_proc_idx]\n",
    "    spans_list = [_intent_spans(cleaned[i]) for i in to_proc_idx]\n",
    "\n",
    "    if to_proc_idx:\n",
    "        for doc, i, spans in zip(nlp.pipe(to_proc_texts, batch_size=256, n_process=1), to_proc_idx, spans_list):\n",
    "            near = _persons_near_spans(doc, spans, window=50)\n",
    "            if near:\n",
    "                results[i] = _dedupe_names(near)\n",
    "\n",
    "    # 2) LLM fallback for remaining intent rows\n",
    "    llm_idxs = [i for i in to_proc_idx if not results[i]]\n",
    "\n",
    "    def _llm_extract_names_from_intent(text: str, model_name: str = \"gemma:2b\") -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "        A YouTube comment suggests future PODCAST GUESTS (e.g., \"bring X on\", \"invite Y\", \"have Z back\").\n",
    "        Extract ONLY the people the viewer wants on the show (ignore examples, comparisons, or negatives like \"don't invite\").\n",
    "        Return JSON array ONLY, e.g. [\"Jordan Peterson\",\"Dr Eric Berg\"].\n",
    "        Comment: \"{text}\"\n",
    "        \"\"\".strip()\n",
    "        try:\n",
    "            resp = ollama.chat(model=model_name, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "            lst = json.loads(resp[\"message\"][\"content\"])\n",
    "            if not isinstance(lst, list): return []\n",
    "            cleaned = [_norm_name(str(x)) for x in lst]\n",
    "            cleaned = [x for x in cleaned if x]\n",
    "            return _dedupe_names(cleaned)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    if llm_idxs:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = {ex.submit(_llm_extract_names_from_intent, cleaned[i], llm_model): i for i in llm_idxs}\n",
    "            for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Guests LLM\"):\n",
    "                i = futs[fut]\n",
    "                try: results[i] = fut.result()\n",
    "                except Exception: results[i] = []\n",
    "\n",
    "    # 3) Title fallback (only when intent present and still empty)\n",
    "    for i in to_proc_idx:\n",
    "        if not results[i]:\n",
    "            t = titles[i] if i < len(titles) else \"\"\n",
    "            if t and TITLE_CUE_RE.search(t):\n",
    "                doc_t = nlp(t)\n",
    "                cand = [_norm_name(ent.text) for ent in doc_t.ents if ent.label_ == \"PERSON\"]\n",
    "                cand = [c for c in cand if c]\n",
    "                results[i] = _dedupe_names(cand)\n",
    "\n",
    "    # Non-intent rows (or negative intent) stay empty\n",
    "    return results\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7) >>> INTENT-DRIVEN TOPIC REQUESTS (span mining + light filters) <<<\n",
    "# =====================================================================\n",
    "\n",
    "BAD_TOPIC_TOKENS = {\"please\",\"thanks\",\"thank you\",\"now\",\"again\",\"next\",\"today\",\"tomorrow\",\"idk\",\"intro\",\"something\",\"someone\"}\n",
    "CHEMICALS = {\"nicotine\",\"propylene glycol\"}\n",
    "MAX_TOPIC_WORDS = 4\n",
    "\n",
    "# Words that make a topic too generic unless explicitly allowed (keep if in allowlist)\n",
    "GENERIC_TOPICS = {\"science\",\"health\",\"ai\",\"tech\",\"technology\",\"business\",\"politics\",\"finance\",\"economics\"}\n",
    "TOPIC_ALLOWLIST = {\"mental health\",\"nutrition\",\"longevity\",\"ai safety\",\"ai at work\",\"relationships\"}\n",
    "\n",
    "def _clean_topic_fragment(s: str) -> List[str]:\n",
    "    s = s.strip(\" .,!?:;|/\\\\[]()\\\"'\")\n",
    "    if not s: return []\n",
    "    parts = re.split(r\"\\s+(?:and|or)\\s+|,|/|;\", s)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        p2 = re.sub(r\"@\\w+|http\\S+\", \"\", p)\n",
    "        p2 = re.sub(r\"[^A-Za-z0-9'\\-\\s]\", \" \", p2)\n",
    "        p2 = re.sub(r\"\\s+\", \" \", p2).strip().lower()\n",
    "        if not p2: continue\n",
    "        if p2 in CHEMICALS or p2 in BAD_TOPIC_TOKENS: continue\n",
    "        if p2 in GENERIC_TOPICS and p2 not in TOPIC_ALLOWLIST: continue\n",
    "        if 1 <= len(p2.split()) <= MAX_TOPIC_WORDS:\n",
    "            out.append(p2)\n",
    "    # dedupe\n",
    "    uniq, seen = [], set()\n",
    "    for x in out:\n",
    "        if x not in seen:\n",
    "            uniq.append(x); seen.add(x)\n",
    "    return uniq\n",
    "\n",
    "def _noun_phrases_within(span_text: str) -> List[str]:\n",
    "    doc = nlp(span_text)\n",
    "    chunks = []\n",
    "    for ch in doc.noun_chunks:\n",
    "        t = ch.text.strip()\n",
    "        t = re.sub(r\"[^A-Za-z0-9'\\-\\s]\", \" \", t)\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
    "        if not t: continue\n",
    "        if t in CHEMICALS or t in BAD_TOPIC_TOKENS: continue\n",
    "        if t in GENERIC_TOPICS and t not in TOPIC_ALLOWLIST: continue\n",
    "        if 1 <= len(t.split()) <= MAX_TOPIC_WORDS:\n",
    "            chunks.append(t)\n",
    "    chunks.sort(key=lambda s: len(s.split()), reverse=True)\n",
    "    uniq, seen = [], set()\n",
    "    for x in chunks:\n",
    "        if x not in seen:\n",
    "            uniq.append(x); seen.add(x)\n",
    "    return uniq[:5]\n",
    "\n",
    "def _llm_normalize_topics_from_intent(text: str, model_name: str = \"gemma:2b\") -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "    A YouTube comment requests PODCAST TOPICS (e.g., \"talk about X\", \"do an episode on Y\").\n",
    "    Extract short, specific topic phrases (1â€“4 words, lower-case) the viewer wants in future episodes.\n",
    "    Ignore vague praise or general chatter.\n",
    "    Return JSON array only. Example: [\"mental health in men\",\"seed oils\",\"ai at work\"]\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\".strip()\n",
    "    try:\n",
    "        resp = ollama.chat(model=model_name, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        lst = json.loads(resp[\"message\"][\"content\"])\n",
    "        if not isinstance(lst, list): return []\n",
    "        out, seen = [], set()\n",
    "        for x in lst:\n",
    "            s = re.sub(r\"[^A-Za-z0-9'\\-\\s]\", \" \", str(x)).strip().lower()\n",
    "            s = re.sub(r\"\\s+\", \" \", s)\n",
    "            if not s: continue\n",
    "            if s in CHEMICALS or s in BAD_TOPIC_TOKENS: continue\n",
    "            if s in GENERIC_TOPICS and s not in TOPIC_ALLOWLIST: continue\n",
    "            if 1 <= len(s.split()) <= MAX_TOPIC_WORDS and s not in seen:\n",
    "                out.append(s); seen.add(s)\n",
    "        return out[:5]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_topics_batch(texts: List[str], max_workers: int = 8, llm_model: str = \"gemma:2b\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Intent-first flow (compute-light):\n",
    "      â€¢ Must match topic intent.\n",
    "      â€¢ Mine the span after the trigger; noun-chunk in that span; clean & dedupe.\n",
    "      â€¢ LLM only if we still got nothing.\n",
    "    \"\"\"\n",
    "    cleaned = [clean_comment(t) for t in texts]\n",
    "    results = [[] for _ in cleaned]\n",
    "\n",
    "    # Which rows have topic-request intent?\n",
    "    intent_mask = [bool(TOPIC_INTENT.search(t or \"\")) for t in cleaned]\n",
    "\n",
    "    # 1) span capture + noun phrase mining for intent rows (batch pipe for speed in long runs)\n",
    "    spans_idx = [i for i, ok in enumerate(intent_mask) if ok]\n",
    "    spans_texts = []\n",
    "    spans_per_idx = []\n",
    "\n",
    "    for i in spans_idx:\n",
    "        spans = [m.group(1) for m in TOPIC_CAPTURE.finditer(cleaned[i])]\n",
    "        if spans:\n",
    "            spans_texts.extend(spans)\n",
    "            spans_per_idx.append((i, len(spans)))\n",
    "        else:\n",
    "            spans_per_idx.append((i, 0))\n",
    "\n",
    "    if spans_texts:\n",
    "        mined = []\n",
    "        for doc in nlp.pipe(spans_texts, batch_size=256, n_process=1):\n",
    "            mined.append(_noun_phrases_within(doc.text))\n",
    "        cursor = 0\n",
    "        for i, nspans in spans_per_idx:\n",
    "            if nspans == 0:\n",
    "                continue\n",
    "            collected = []\n",
    "            for _ in range(nspans):\n",
    "                collected.extend(mined[cursor])\n",
    "                cursor += 1\n",
    "            # plus rule-clean fragments\n",
    "            for s in [m.group(1) for m in TOPIC_CAPTURE.finditer(cleaned[i])]:\n",
    "                collected.extend(_clean_topic_fragment(s))\n",
    "            uniq, seen = [], set()\n",
    "            for tp in collected:\n",
    "                if tp not in seen:\n",
    "                    uniq.append(tp); seen.add(tp)\n",
    "            results[i] = uniq[:5]\n",
    "\n",
    "    # 2) LLM normalization when still empty but intent exists\n",
    "    llm_idxs = [i for i, ok in enumerate(intent_mask) if ok and not results[i]]\n",
    "    if llm_idxs:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = {ex.submit(_llm_normalize_topics_from_intent, cleaned[i], llm_model): i for i in llm_idxs}\n",
    "            for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Topics LLM\"):\n",
    "                i = futs[fut]\n",
    "                try: results[i] = fut.result()\n",
    "                except Exception: results[i] = []\n",
    "\n",
    "    # Final dedupe per row\n",
    "    final = []\n",
    "    for lst in results:\n",
    "        uniq, seen = [], set()\n",
    "        for t in lst:\n",
    "            if t not in seen:\n",
    "                uniq.append(t); seen.add(t)\n",
    "        final.append(uniq)\n",
    "    return final\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8) ENSEMBLE DRIVER\n",
    "# -----------------------------\n",
    "def ensemble_with_llm_gate(texts: List[str],\n",
    "                           llm_workers: int = 8,\n",
    "                           llm_model: str = \"gemma:2b\") -> Dict[str, np.ndarray]:\n",
    "    return ensemble_scores_fast(texts, call_llm_if_uncertain=True,\n",
    "                                llm_max_workers=llm_workers, llm_model=llm_model)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 9) APPLY TO DATAFRAME (CACHED)\n",
    "# -----------------------------\n",
    "def apply_sentiment_fast(df: pd.DataFrame,\n",
    "                         text_col: str = \"comment_text\",\n",
    "                         id_col: str = \"comment_id\",\n",
    "                         like_col: str = \"comment_like_count\",\n",
    "                         title_col: Optional[str] = \"video_title\",\n",
    "                         use_cache: bool = True,\n",
    "                         llm_workers: int = 8,\n",
    "                         llm_model: str = \"gemma:2b\",\n",
    "                         run_guest_topic: bool = True) -> pd.DataFrame:\n",
    "\n",
    "    assert text_col in df.columns, f\"'{text_col}' not in df\"\n",
    "    assert id_col in df.columns, f\"'{id_col}' not in df\"\n",
    "\n",
    "    _ensure_cache()\n",
    "    out = df.copy()\n",
    "    out[text_col] = out[text_col].fillna(\"\").astype(str)\n",
    "    out[\"cleaned_text\"] = out[text_col].map(clean_comment)\n",
    "\n",
    "    # sentiment cache prep\n",
    "    out[\"__text_hash\"] = out[\"cleaned_text\"].map(sha1)\n",
    "    ids = out[id_col].astype(str).tolist()\n",
    "    id_to_hash = dict(zip(ids, out[\"__text_hash\"].tolist()))\n",
    "    cached_sent = cache_get_sentiments(ids, id_to_hash) if use_cache else {}\n",
    "    cached_mask_sent = out[id_col].astype(str).isin(cached_sent.keys())\n",
    "\n",
    "    for col in [\"sentiment_p_pos\",\"sentiment_bucket\",\"p_pos_llm\",\"p_pos_hf\",\"p_neg_hf\"]:\n",
    "        out[col] = np.nan if col!=\"sentiment_bucket\" else None\n",
    "\n",
    "    if cached_sent:\n",
    "        idx = out.index[cached_mask_sent]\n",
    "        for i in idx:\n",
    "            cid = str(out.at[i, id_col]); row = cached_sent[cid]\n",
    "            out.at[i,\"sentiment_p_pos\"] = row[\"p_pos\"]\n",
    "            out.at[i,\"sentiment_bucket\"] = row[\"bucket\"]\n",
    "            out.at[i,\"p_pos_llm\"] = row[\"p_pos_llm\"]\n",
    "            out.at[i,\"p_pos_hf\"] = row[\"p_pos_hf\"]\n",
    "            out.at[i,\"p_neg_hf\"] = row[\"p_neg_hf\"]\n",
    "\n",
    "    need = out.index[~cached_mask_sent].tolist()\n",
    "    if need:\n",
    "        texts = out.loc[need, \"cleaned_text\"].tolist()\n",
    "        scores = ensemble_with_llm_gate(texts, llm_workers, llm_model)\n",
    "        out.loc[need, \"p_pos_hf\"] = scores[\"p_pos_hf\"]\n",
    "        out.loc[need, \"p_neg_hf\"] = scores[\"p_neg_hf\"]\n",
    "        out.loc[need, \"p_pos_llm\"] = scores[\"p_pos_llm\"]\n",
    "        out.loc[need, \"sentiment_p_pos\"] = scores[\"p_pos\"]\n",
    "        out.loc[need, \"sentiment_bucket\"] = [bucket_from_p(v) for v in scores[\"p_pos\"]]\n",
    "\n",
    "        if use_cache:\n",
    "            to_cache = []\n",
    "            for i in need:\n",
    "                to_cache.append((\n",
    "                    str(out.at[i, id_col]),\n",
    "                    out.at[i, \"__text_hash\"],\n",
    "                    float(out.at[i, \"sentiment_p_pos\"]),\n",
    "                    str(out.at[i, \"sentiment_bucket\"]),\n",
    "                    float(out.at[i, \"p_pos_llm\"]),\n",
    "                    float(out.at[i, \"p_pos_hf\"]),\n",
    "                    float(out.at[i, \"p_neg_hf\"]),\n",
    "                ))\n",
    "            cache_put_sentiments(to_cache)\n",
    "\n",
    "    # impact weighting\n",
    "    if like_col in out.columns:\n",
    "        out[like_col] = out[like_col].fillna(0).astype(int)\n",
    "        out[\"comment_weight\"] = out[like_col].map(like_weight)\n",
    "    else:\n",
    "        out[\"comment_weight\"] = 1.0\n",
    "    out[\"impact_weighted_sentiment\"] = out[\"sentiment_p_pos\"].astype(float) * out[\"comment_weight\"].astype(float)\n",
    "\n",
    "    # guests/topics (intent-driven) + caching\n",
    "    if run_guest_topic:\n",
    "        cached_meta = cache_get_meta(ids, id_to_hash) if use_cache else {}\n",
    "        cached_mask_meta = out[id_col].astype(str).isin(cached_meta.keys())\n",
    "        out[\"guest_mentions\"]  = [[] for _ in range(len(out))]\n",
    "        out[\"topic_requests\"]  = [[] for _ in range(len(out))]\n",
    "\n",
    "        if cached_meta:\n",
    "            idx = out.index[cached_mask_meta]\n",
    "            for i in idx:\n",
    "                cid = str(out.at[i, id_col]); row = cached_meta[cid]\n",
    "                out.at[i, \"guest_mentions\"] = row[\"guest_mentions\"]\n",
    "                out.at[i, \"topic_requests\"] = row[\"topic_requests\"]\n",
    "\n",
    "        need_meta = out.index[~cached_mask_meta].tolist()\n",
    "        if need_meta:\n",
    "            texts = out.loc[need_meta, \"cleaned_text\"].tolist()\n",
    "            titles = out[title_col].tolist() if title_col and title_col in out.columns else [\"\"]*len(out)\n",
    "\n",
    "            guests_lists = extract_guests_batch(texts, [titles[i] for i in need_meta],\n",
    "                                                max_workers=llm_workers, llm_model=llm_model)\n",
    "            topics_lists = extract_topics_batch(texts, max_workers=llm_workers, llm_model=llm_model)\n",
    "\n",
    "            for i, glst in zip(need_meta, guests_lists):\n",
    "                out.at[i, \"guest_mentions\"] = glst\n",
    "            for i, tlst in zip(need_meta, topics_lists):\n",
    "                out.at[i, \"topic_requests\"] = tlst\n",
    "\n",
    "            if use_cache:\n",
    "                to_meta = []\n",
    "                for i in need_meta:\n",
    "                    to_meta.append((\n",
    "                        str(out.at[i, id_col]),\n",
    "                        out.at[i, \"__text_hash\"],\n",
    "                        out.at[i, \"guest_mentions\"],\n",
    "                        out.at[i, \"topic_requests\"],\n",
    "                    ))\n",
    "                cache_put_meta(to_meta)\n",
    "\n",
    "    out.drop(columns=[\"__text_hash\"], inplace=True, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Example\n",
    "# ===========================\n",
    "# out = apply_sentiment_fast(\n",
    "#     df,\n",
    "#     text_col=\"comment_text\",\n",
    "#     id_col=\"comment_id\",\n",
    "#     like_col=\"comment_like_count\",\n",
    "#     title_col=\"video_title\",\n",
    "#     use_cache=True,\n",
    "#     llm_workers=8,\n",
    "#     llm_model=\"gemma:2b\",\n",
    "#     run_guest_topic=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4f6f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM (parallel): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:35<00:00,  1.31it/s]\n",
      "Guests LLM: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "out = apply_sentiment_fast(df,\n",
    "     text_col=\"comment_text\",\n",
    "     id_col=\"comment_id\",\n",
    "     like_col=\"comment_like_count\",\n",
    "     title_col=\"video_title\",\n",
    "     use_cache=True,\n",
    "     llm_workers=8,\n",
    "     llm_model=\"gemma:2b\",\n",
    "     run_guest_topic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2516d154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>guest_list</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Topic_Category</th>\n",
       "      <th>sentiment_p_pos</th>\n",
       "      <th>sentiment_bucket</th>\n",
       "      <th>p_pos_llm</th>\n",
       "      <th>p_pos_hf</th>\n",
       "      <th>p_neg_hf</th>\n",
       "      <th>comment_weight</th>\n",
       "      <th>impact_weighted_sentiment</th>\n",
       "      <th>guest_mentions</th>\n",
       "      <th>topic_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>Hik6OY-nk4c</td>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely &amp; Sexless Society!</td>\n",
       "      <td>Dr Jordan Peterson is a world-renowned former Professor of Psychology at the University of Toron...</td>\n",
       "      <td>2025-01-13T08:00:19Z</td>\n",
       "      <td>2046763</td>\n",
       "      <td>54184</td>\n",
       "      <td>6392</td>\n",
       "      <td>UgzCgbdzte3zTTQex6Z4AaABAg</td>\n",
       "      <td>This was very eye opening. I was sort of like this idiot that Jordan was trying to help.</td>\n",
       "      <td>@JarreVonDuck</td>\n",
       "      <td>UCQMQlxaRQf04JUkceJ6obpw</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-23T21:52:03Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "      <td>This was very eye opening. I was sort of like this idiot that Jordan was trying to help.</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0.399823</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333039</td>\n",
       "      <td>0.268008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399823</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         channel_name                channel_id     video_id  \\\n",
       "0  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  Hik6OY-nk4c   \n",
       "\n",
       "                                                                                   video_title  \\\n",
       "0  Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!   \n",
       "\n",
       "                                                                                     video_description  \\\n",
       "0  Dr Jordan Peterson is a world-renowned former Professor of Psychology at the University of Toron...   \n",
       "\n",
       "     video_published_at  view_count  video_like_count  comment_count  \\\n",
       "0  2025-01-13T08:00:19Z     2046763             54184           6392   \n",
       "\n",
       "                   comment_id  \\\n",
       "0  UgzCgbdzte3zTTQex6Z4AaABAg   \n",
       "\n",
       "                                                                               comment_text  \\\n",
       "0  This was very eye opening. I was sort of like this idiot that Jordan was trying to help.   \n",
       "\n",
       "          author                 author_id  comment_like_count  \\\n",
       "0  @JarreVonDuck  UCQMQlxaRQf04JUkceJ6obpw                   0   \n",
       "\n",
       "   comment_published_at  is_pinned  is_reply parent_comment_id  \\\n",
       "0  2025-01-23T21:52:03Z      False     False               NaN   \n",
       "\n",
       "             guest_list  \\\n",
       "0  [Dr Jordan Peterson]   \n",
       "\n",
       "                                                                               cleaned_text  \\\n",
       "0  This was very eye opening. I was sort of like this idiot that Jordan was trying to help.   \n",
       "\n",
       "  Topic_Category  sentiment_p_pos sentiment_bucket  p_pos_llm  p_pos_hf  \\\n",
       "0   relationship         0.399823          Neutral        0.5  0.333039   \n",
       "\n",
       "   p_neg_hf  comment_weight  impact_weighted_sentiment guest_mentions  \\\n",
       "0  0.268008             1.0                   0.399823             []   \n",
       "\n",
       "  topic_requests  \n",
       "0             []  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32640809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['channel_name', 'channel_id', 'video_id', 'video_title',\n",
       "       'video_description', 'video_published_at', 'view_count',\n",
       "       'video_like_count', 'comment_count', 'comment_id', 'comment_text',\n",
       "       'author', 'author_id', 'comment_like_count', 'comment_published_at',\n",
       "       'is_pinned', 'is_reply', 'parent_comment_id', 'guest_list',\n",
       "       'cleaned_text', 'Topic_Category', 'sentiment_p_pos', 'sentiment_bucket',\n",
       "       'p_pos_llm', 'p_pos_hf', 'p_neg_hf', 'comment_weight',\n",
       "       'impact_weighted_sentiment', 'guest_mentions', 'topic_requests'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f021bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>guest_list</th>\n",
       "      <th>Topic_Category</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment_p_pos</th>\n",
       "      <th>sentiment_bucket</th>\n",
       "      <th>p_pos_llm</th>\n",
       "      <th>p_pos_hf</th>\n",
       "      <th>p_neg_hf</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_weight</th>\n",
       "      <th>impact_weighted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely &amp; Sexless Society!</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "      <td>relationship</td>\n",
       "      <td>This was very eye opening. I was sort of like this idiot that Jordan was trying to help.</td>\n",
       "      <td>0.399823</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333039</td>\n",
       "      <td>0.268008</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.</td>\n",
       "      <td>[Dr. Sara Szal]</td>\n",
       "      <td>health</td>\n",
       "      <td>You cant even spell hormone. Shush.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.878349</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exercise &amp; Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!</td>\n",
       "      <td>[Dr Stacy Sims]</td>\n",
       "      <td>health</td>\n",
       "      <td>Im 40 and I agree with this list it is working for meeeee I feel great!</td>\n",
       "      <td>0.985389</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.985389</td>\n",
       "      <td>0.985389</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.182467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Body Language Expert Explains Why People Dislike You</td>\n",
       "      <td>[Vanessa Van Edwards, Steven]</td>\n",
       "      <td>mental health / psychology</td>\n",
       "      <td>Pure gold! Seeing a lot of confusion in the comments. You have to realise that all the advice is...</td>\n",
       "      <td>0.127034</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.128390</td>\n",
       "      <td>0.426005</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.152441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exercise &amp; Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!</td>\n",
       "      <td>[Dr Stacy Sims]</td>\n",
       "      <td>health</td>\n",
       "      <td>i tried intermittent fasting and my period stopped coming.. im still trying to fix it nobody war...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.897784</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely &amp; Sexless Society!</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "      <td>relationship</td>\n",
       "      <td>@'s not about disagreement though. Its literally saying or doing nothing when you see others do ...</td>\n",
       "      <td>0.066119</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.741934</td>\n",
       "      <td>54</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.132237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!</td>\n",
       "      <td>[Master Shi Heng Yi]</td>\n",
       "      <td>health</td>\n",
       "      <td>Side note: What is it with spiritual people and tapping on tables? Is there an effect created by...</td>\n",
       "      <td>0.216586</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027644</td>\n",
       "      <td>0.137783</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.216586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!</td>\n",
       "      <td>[Master Shi Heng Yi]</td>\n",
       "      <td>health</td>\n",
       "      <td>Steven, your questions are  amazingly on point. Wise, smart, thoughtful, well timed. Congratulat...</td>\n",
       "      <td>0.951639</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.986065</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Body Language Expert Explains Why People Dislike You</td>\n",
       "      <td>[Vanessa Van Edwards, Steven]</td>\n",
       "      <td>mental health / psychology</td>\n",
       "      <td>I prefer to be wary of first impressions and downplay their importance. How realistic are they? ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.058139</td>\n",
       "      <td>0.058139</td>\n",
       "      <td>0.445123</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Body Language Expert Explains Why People Dislike You</td>\n",
       "      <td>[Vanessa Van Edwards, Steven]</td>\n",
       "      <td>mental health / psychology</td>\n",
       "      <td>Jokes on you when I check my phone I raise it in front of my face and don't scrunch up in defeat...</td>\n",
       "      <td>0.329028</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.329028</td>\n",
       "      <td>0.329028</td>\n",
       "      <td>0.079858</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          video_title  \\\n",
       "0         Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!   \n",
       "1    Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.   \n",
       "2  Exercise & Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!   \n",
       "3                                                Body Language Expert Explains Why People Dislike You   \n",
       "4  Exercise & Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!   \n",
       "5         Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!   \n",
       "6  Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!   \n",
       "7  Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!   \n",
       "8                                                Body Language Expert Explains Why People Dislike You   \n",
       "9                                                Body Language Expert Explains Why People Dislike You   \n",
       "\n",
       "                      guest_list              Topic_Category  \\\n",
       "0           [Dr Jordan Peterson]                relationship   \n",
       "1                [Dr. Sara Szal]                      health   \n",
       "2                [Dr Stacy Sims]                      health   \n",
       "3  [Vanessa Van Edwards, Steven]  mental health / psychology   \n",
       "4                [Dr Stacy Sims]                      health   \n",
       "5           [Dr Jordan Peterson]                relationship   \n",
       "6           [Master Shi Heng Yi]                      health   \n",
       "7           [Master Shi Heng Yi]                      health   \n",
       "8  [Vanessa Van Edwards, Steven]  mental health / psychology   \n",
       "9  [Vanessa Van Edwards, Steven]  mental health / psychology   \n",
       "\n",
       "                                                                                          cleaned_text  \\\n",
       "0             This was very eye opening. I was sort of like this idiot that Jordan was trying to help.   \n",
       "1                                                                  You cant even spell hormone. Shush.   \n",
       "2                              Im 40 and I agree with this list it is working for meeeee I feel great!   \n",
       "3  Pure gold! Seeing a lot of confusion in the comments. You have to realise that all the advice is...   \n",
       "4  i tried intermittent fasting and my period stopped coming.. im still trying to fix it nobody war...   \n",
       "5  @'s not about disagreement though. Its literally saying or doing nothing when you see others do ...   \n",
       "6  Side note: What is it with spiritual people and tapping on tables? Is there an effect created by...   \n",
       "7  Steven, your questions are  amazingly on point. Wise, smart, thoughtful, well timed. Congratulat...   \n",
       "8  I prefer to be wary of first impressions and downplay their importance. How realistic are they? ...   \n",
       "9  Jokes on you when I check my phone I raise it in front of my face and don't scrunch up in defeat...   \n",
       "\n",
       "   sentiment_p_pos sentiment_bucket  p_pos_llm  p_pos_hf  p_neg_hf  \\\n",
       "0         0.399823          Neutral   0.500000  0.333039  0.268008   \n",
       "1         0.000000         Negative   0.011648  0.011648  0.878349   \n",
       "2         0.985389         Positive   0.985389  0.985389  0.003019   \n",
       "3         0.127034         Negative   0.625000  0.128390  0.426005   \n",
       "4         0.000000         Negative   0.005976  0.005976  0.897784   \n",
       "5         0.066119         Negative   0.650000  0.010198  0.741934   \n",
       "6         0.216586         Negative   0.500000  0.027644  0.137783   \n",
       "7         0.951639         Positive   0.900000  0.986065  0.003411   \n",
       "8         0.000000         Negative   0.058139  0.058139  0.445123   \n",
       "9         0.329028         Negative   0.329028  0.329028  0.079858   \n",
       "\n",
       "   comment_like_count  comment_weight  impact_weighted_sentiment  \n",
       "0                   0             1.0                   0.399823  \n",
       "1                   0             1.0                   0.000000  \n",
       "2                   2             1.2                   1.182467  \n",
       "3                   3             1.2                   0.152441  \n",
       "4                   0             1.0                   0.000000  \n",
       "5                  54             2.0                   0.132237  \n",
       "6                   0             1.0                   0.216586  \n",
       "7                   0             1.0                   0.951639  \n",
       "8                   0             1.0                   0.000000  \n",
       "9                   0             1.0                   0.329028  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[['video_title', 'guest_list','Topic_Category','cleaned_text', 'sentiment_p_pos', 'sentiment_bucket', 'p_pos_llm', 'p_pos_hf', 'p_neg_hf', 'comment_like_count', 'comment_weight', 'impact_weighted_sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b835e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_bucket\n",
      "Negative    53\n",
      "Positive    32\n",
      "Neutral     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "comment_weight\n",
      "1.0    70\n",
      "1.2    26\n",
      "2.0     3\n",
      "4.0     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "guest_mentions\n",
      "[]    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "topic_requests\n",
      "[]    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(out['sentiment_bucket'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(out['comment_weight'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(out['guest_mentions'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(out['topic_requests'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9670e",
   "metadata": {},
   "source": [
    "- Fix logic for guest_mentions and topic request\n",
    "- Guest mention: prompt the llm or NLP script to look for \"bring xx on\" or \"invite xx\" or \"bring back\" --> Variants like this\n",
    "- Topic request: \"more on xx\", \"discuss xx\", \"more of xx\" -> variants like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312354e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-diary-of-a-ceo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
