{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6607339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/riadanas/Desktop/MLE Diary of a CEO/data/raw/_test_3.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346ecf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ad723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>jFlnRBO8mcg</td>\n",
       "      <td>The Savings Expert: Passive Income Is A Scam! ...</td>\n",
       "      <td>Morgan Housel, global expert on personal finan...</td>\n",
       "      <td>2025-10-06T07:00:44Z</td>\n",
       "      <td>1023328</td>\n",
       "      <td>23390</td>\n",
       "      <td>2241</td>\n",
       "      <td>UgxZ7109QlZor9DvxaN4AaABAg</td>\n",
       "      <td>The idea that our culture prioritizes freedom ...</td>\n",
       "      <td>@michaelcupper</td>\n",
       "      <td>UCYu6S_dTgdQCop5mVZenYvw</td>\n",
       "      <td>2508</td>\n",
       "      <td>2025-10-08T09:51:51Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>jFlnRBO8mcg</td>\n",
       "      <td>The Savings Expert: Passive Income Is A Scam! ...</td>\n",
       "      <td>Morgan Housel, global expert on personal finan...</td>\n",
       "      <td>2025-10-06T07:00:44Z</td>\n",
       "      <td>1023328</td>\n",
       "      <td>23390</td>\n",
       "      <td>2241</td>\n",
       "      <td>Ugz0jfygK2CpiaGjZB54AaABAg</td>\n",
       "      <td>Iâ€™ve read almost every book on making money bu...</td>\n",
       "      <td>@rodey-f7z3w</td>\n",
       "      <td>UCQIKp48s3SOe7dYMBuCsBgg</td>\n",
       "      <td>1191</td>\n",
       "      <td>2025-10-08T03:09:24Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         channel_name                channel_id     video_id  \\\n",
       "0  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  jFlnRBO8mcg   \n",
       "1  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  jFlnRBO8mcg   \n",
       "\n",
       "                                         video_title  \\\n",
       "0  The Savings Expert: Passive Income Is A Scam! ...   \n",
       "1  The Savings Expert: Passive Income Is A Scam! ...   \n",
       "\n",
       "                                   video_description    video_published_at  \\\n",
       "0  Morgan Housel, global expert on personal finan...  2025-10-06T07:00:44Z   \n",
       "1  Morgan Housel, global expert on personal finan...  2025-10-06T07:00:44Z   \n",
       "\n",
       "   view_count  video_like_count  comment_count                  comment_id  \\\n",
       "0     1023328             23390           2241  UgxZ7109QlZor9DvxaN4AaABAg   \n",
       "1     1023328             23390           2241  Ugz0jfygK2CpiaGjZB54AaABAg   \n",
       "\n",
       "                                        comment_text          author  \\\n",
       "0  The idea that our culture prioritizes freedom ...  @michaelcupper   \n",
       "1  Iâ€™ve read almost every book on making money bu...    @rodey-f7z3w   \n",
       "\n",
       "                  author_id  comment_like_count  comment_published_at  \\\n",
       "0  UCYu6S_dTgdQCop5mVZenYvw                2508  2025-10-08T09:51:51Z   \n",
       "1  UCQIKp48s3SOe7dYMBuCsBgg                1191  2025-10-08T03:09:24Z   \n",
       "\n",
       "   is_pinned  is_reply parent_comment_id  \n",
       "0      False     False               NaN  \n",
       "1      False     False               NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e4da4",
   "metadata": {},
   "source": [
    "## Guest Name Processing - GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3447cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_guest_names_openrouter(description: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract true podcast guest names from a YouTube description using OpenRouter (Claude 3.5 / GPT-4-mini).\n",
    "    Ignores names used as references or examples.\n",
    "    \"\"\"\n",
    "    if not isinstance(description, str) or not description.strip():\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a podcast metadata assistant.\n",
    "\n",
    "    Task:\n",
    "    - Read the YouTube video description carefully.\n",
    "    - Identify ONLY the actual guest(s) who appear in the episode or are directly interviewed.\n",
    "    - Ignore people mentioned just as examples, comparisons, or references (e.g., Warren Buffett, Elon Musk).\n",
    "    - If multiple guests appear, include all of them.\n",
    "    - Preserve professional titles (e.g., \"Dr\", \"Prof\", \"Sir\") if present.\n",
    "    - Return a clean JSON list of guest names, for example:\n",
    "      [\"Morgan Housel\"]\n",
    "      or [\"Dr Andrew Huberman\", \"Lex Fridman\"]\n",
    "    - If no guest is clearly identified, return an empty list [].\n",
    "\n",
    "    Description:\n",
    "    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"anthropic/claude-3.5-sonnet\",  # you can change to \"openai/gpt-4o-mini\"\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "\n",
    "        content = completion.choices[0].message.content.strip()\n",
    "\n",
    "        # Try parsing JSON\n",
    "        try:\n",
    "            result = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r'\\[(.*?)\\]', content)\n",
    "            if match:\n",
    "                inner = match.group(1)\n",
    "                result = [n.strip().strip('\"').strip() for n in inner.split(\",\") if n.strip()]\n",
    "            else:\n",
    "                result = re.findall(r\"(?:Dr\\.?|Prof\\.?|Mr\\.?|Ms\\.?)?\\s?[A-Z][a-z]+(?:\\s[A-Z][a-z]+)+\", content)\n",
    "\n",
    "        if isinstance(result, str):\n",
    "            result = [result]\n",
    "        result = [r.strip() for r in result if r.strip()]\n",
    "        result = list(set(result))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing description: {e}\")\n",
    "        result = []\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ðŸ” Apply once per unique video_id\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def assign_guest_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply guest extraction once per unique video_id.\n",
    "    Adds a 'guest_list' column to the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create mapping: video_id â†’ guest list\n",
    "    mapping = {}\n",
    "    unique_videos = df.drop_duplicates(subset=\"video_id\")[[\"video_id\", \"video_description\"]]\n",
    "\n",
    "    for _, row in unique_videos.iterrows():\n",
    "        vid = row[\"video_id\"]\n",
    "        desc = row[\"video_description\"]\n",
    "        guests = get_guest_names_openrouter(desc)\n",
    "        mapping[vid] = guests\n",
    "\n",
    "    # Map results back to main DataFrame\n",
    "    df[\"guest_list\"] = df[\"video_id\"].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc3b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assign_guest_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3b47e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guest_list\n",
       "[Morgan Housel]    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['guest_list'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a58d885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>guest_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>jFlnRBO8mcg</td>\n",
       "      <td>The Savings Expert: Passive Income Is A Scam! ...</td>\n",
       "      <td>Morgan Housel, global expert on personal finan...</td>\n",
       "      <td>2025-10-06T07:00:44Z</td>\n",
       "      <td>1023328</td>\n",
       "      <td>23390</td>\n",
       "      <td>2241</td>\n",
       "      <td>UgxZ7109QlZor9DvxaN4AaABAg</td>\n",
       "      <td>The idea that our culture prioritizes freedom ...</td>\n",
       "      <td>@michaelcupper</td>\n",
       "      <td>UCYu6S_dTgdQCop5mVZenYvw</td>\n",
       "      <td>2508</td>\n",
       "      <td>2025-10-08T09:51:51Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>jFlnRBO8mcg</td>\n",
       "      <td>The Savings Expert: Passive Income Is A Scam! ...</td>\n",
       "      <td>Morgan Housel, global expert on personal finan...</td>\n",
       "      <td>2025-10-06T07:00:44Z</td>\n",
       "      <td>1023328</td>\n",
       "      <td>23390</td>\n",
       "      <td>2241</td>\n",
       "      <td>Ugz0jfygK2CpiaGjZB54AaABAg</td>\n",
       "      <td>Iâ€™ve read almost every book on making money bu...</td>\n",
       "      <td>@rodey-f7z3w</td>\n",
       "      <td>UCQIKp48s3SOe7dYMBuCsBgg</td>\n",
       "      <td>1191</td>\n",
       "      <td>2025-10-08T03:09:24Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         channel_name                channel_id     video_id  \\\n",
       "0  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  jFlnRBO8mcg   \n",
       "1  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  jFlnRBO8mcg   \n",
       "\n",
       "                                         video_title  \\\n",
       "0  The Savings Expert: Passive Income Is A Scam! ...   \n",
       "1  The Savings Expert: Passive Income Is A Scam! ...   \n",
       "\n",
       "                                   video_description    video_published_at  \\\n",
       "0  Morgan Housel, global expert on personal finan...  2025-10-06T07:00:44Z   \n",
       "1  Morgan Housel, global expert on personal finan...  2025-10-06T07:00:44Z   \n",
       "\n",
       "   view_count  video_like_count  comment_count                  comment_id  \\\n",
       "0     1023328             23390           2241  UgxZ7109QlZor9DvxaN4AaABAg   \n",
       "1     1023328             23390           2241  Ugz0jfygK2CpiaGjZB54AaABAg   \n",
       "\n",
       "                                        comment_text          author  \\\n",
       "0  The idea that our culture prioritizes freedom ...  @michaelcupper   \n",
       "1  Iâ€™ve read almost every book on making money bu...    @rodey-f7z3w   \n",
       "\n",
       "                  author_id  comment_like_count  comment_published_at  \\\n",
       "0  UCYu6S_dTgdQCop5mVZenYvw                2508  2025-10-08T09:51:51Z   \n",
       "1  UCQIKp48s3SOe7dYMBuCsBgg                1191  2025-10-08T03:09:24Z   \n",
       "\n",
       "   is_pinned  is_reply parent_comment_id       guest_list  \n",
       "0      False     False               NaN  [Morgan Housel]  \n",
       "1      False     False               NaN  [Morgan Housel]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf69abbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Morgan Housel, global expert on personal finance, shares powerful lessons on Warren Buffettâ€™s hidden struggles, Elon Muskâ€™s sacrifices, money trauma and financial habits, how to invest wisely, and the psychology behind saving, spending, and success.   Morgan Housel is a partner at Collaborative Fund, former columnist for The Wall Street Journal, and a speaker on investing, saving, spending, and financial independence. He is also the bestselling author of books, such as: â€˜The Psychology of Moneyâ€™'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['video_description'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c769ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the heavy text column to speed up further LLM steps\n",
    "# df = df.drop(columns=[\"video_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27237f5",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab8a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Text Cleaning (light only)\n",
    "# ------------------------------------------------------\n",
    "def clean_comment(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning for comments:\n",
    "    - Remove @mentions\n",
    "    - Remove URLs\n",
    "    - Remove emojis / non-ascii\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    - Keep context words (no lemmatization, no stopword removal yet)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # remove mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    # remove emojis/non-ascii\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # lowercase + strip\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cb8d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The idea that our culture prioritizes freedom ...</td>\n",
       "      <td>the idea that our culture prioritizes freedom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iâ€™ve read almost every book on making money bu...</td>\n",
       "      <td>ive read almost every book on making money but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He who knows that enough is enough will always...</td>\n",
       "      <td>he who knows that enough is enough will always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iâ€™m a doctor and â€˜Secrets To Perfect Healthâ€™ b...</td>\n",
       "      <td>im a doctor and secrets to perfect health by n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The world is a scam ðŸ˜‚</td>\n",
       "      <td>the world is a scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What nobody tells students is that making mone...</td>\n",
       "      <td>what nobody tells students is that making mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Most men think success is all about grinding h...</td>\n",
       "      <td>most men think success is all about grinding h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>People once used to compare themselves to peop...</td>\n",
       "      <td>people once used to compare themselves to peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I am retiring in 2 years at the age of 62 I ha...</td>\n",
       "      <td>i am retiring in 2 years at the age of 62 i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>To me, I DO NOT need to go AWAY to enjoy mysel...</td>\n",
       "      <td>to me, i do not need to go away to enjoy mysel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  The idea that our culture prioritizes freedom ...   \n",
       "1  Iâ€™ve read almost every book on making money bu...   \n",
       "2  He who knows that enough is enough will always...   \n",
       "3  Iâ€™m a doctor and â€˜Secrets To Perfect Healthâ€™ b...   \n",
       "4                              The world is a scam ðŸ˜‚   \n",
       "5  What nobody tells students is that making mone...   \n",
       "6  Most men think success is all about grinding h...   \n",
       "7  People once used to compare themselves to peop...   \n",
       "8  I am retiring in 2 years at the age of 62 I ha...   \n",
       "9  To me, I DO NOT need to go AWAY to enjoy mysel...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  the idea that our culture prioritizes freedom ...  \n",
       "1  ive read almost every book on making money but...  \n",
       "2  he who knows that enough is enough will always...  \n",
       "3  im a doctor and secrets to perfect health by n...  \n",
       "4                                the world is a scam  \n",
       "5  what nobody tells students is that making mone...  \n",
       "6  most men think success is all about grinding h...  \n",
       "7  people once used to compare themselves to peop...  \n",
       "8  i am retiring in 2 years at the age of 62 i ha...  \n",
       "9  to me, i do not need to go away to enjoy mysel...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply light cleaning to the comment_text column\n",
    "df[\"cleaned_text\"] = df[\"comment_text\"].apply(clean_comment)\n",
    "\n",
    "# Preview the results\n",
    "df[[\"comment_text\", \"cleaned_text\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f129b61",
   "metadata": {},
   "source": [
    "## Topic category (llama) -> Once per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04cbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "def get_topic_category(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Ollama to classify the video title into a topic category.\n",
    "    Example categories: health, mental health, productivity, finance, relationships, entrepreneurship, other.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Categorize the following YouTube video title into ONE broad category:\n",
    "    - health\n",
    "    - mental health\n",
    "    - productivity\n",
    "    - finance\n",
    "    - relationships\n",
    "    - entrepreneurship\n",
    "    - Religion / Spirituality\n",
    "    - Technology\n",
    "    - Education\n",
    "    - Lifestyle\n",
    "    - Entertainment\n",
    "    - other\n",
    "\n",
    "    Title: \"{title}\"\n",
    "\n",
    "    Return only the category name, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2:3b\",  # you can swap to another local model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a391d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Extract unique video_id/title pairs\n",
    "video_meta = df[[\"video_id\", \"video_title\"]].drop_duplicates()\n",
    "\n",
    "# Apply Ollama category classification\n",
    "video_meta[\"Topic_Category\"] = video_meta[\"video_title\"].apply(get_topic_category)\n",
    "\n",
    "# Merge back into main dataframe\n",
    "df = df.merge(video_meta[[\"video_id\", \"Topic_Category\"]], on=\"video_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f90aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic_Category\n",
       "finance    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2b696",
   "metadata": {},
   "source": [
    "## Sentiment analysis (Gemma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb74efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Helpers & resources\n",
    "# -----------------------------\n",
    "NEG_PHRASES = [\n",
    "    r\"\\bnot good\\b\", r\"\\bnot great\\b\", r\"\\bnot helpful\\b\", r\"\\bdon't like\\b\", r\"\\bdont like\\b\",\n",
    "    r\"\\bnot worth\\b\", r\"\\bwaste of time\\b\", r\"\\btoo long\\b\", r\"\\btoo slow\\b\", r\"\\btoo loud\\b\",\n",
    "    r\"\\bmisleading\\b\", r\"\\bclickbait\\b\", r\"\\bbiased\\b\", r\"\\bconfusing\\b\", r\"\\bannoying\\b\",\n",
    "    r\"\\bcringe\\b\", r\"\\bstupid\\b\", r\"\\bdumb\\b\", r\"\\bbad\\b\", r\"\\bawful\\b\", r\"\\bterrible\\b\",\n",
    "    r\"\\buseless\\b\", r\"\\bpointless\\b\", r\"\\bwrong\\b\", r\"\\bpoor (audio|sound|quality)\\b\",\n",
    "    r\"\\birrelevant\\b\", r\"\\boff\\-topic\\b\", r\"\\bproblem\\b\", r\"\\bissue\\b\", r\"\\bdisappoint(ing|ed)\\b\",\n",
    "    r\"\\brude\\b\", r\"\\boffensive\\b\", r\"\\bunfunny\\b\", r\"\\bboring\\b\", r\"\\blazy\\b\", r\"\\btoxic\\b\",\n",
    "    r\"\\bhate\\b\", r\"\\bgarbage\\b\", r\"\\bignorant\\b\", r\"\\bweird\\b\", r\"\\bnegative\\b\", r\"\\bbroken\\b\",\n",
    "    r\"\\bdownvote\\b\", r\"\\bterribly\\b\", r\"\\bdislike\\b\", r\"\\bpathetic\\b\", r\"\\bworse\\b\"\n",
    "]\n",
    "NEG_RE = re.compile(\"|\".join(NEG_PHRASES))\n",
    "\n",
    "SHORT_PRAISE_RE = re.compile(r\"^(nice|cool|great|good|amazing|awesome|wow|love|thanks|perfect)[.!]?$\", re.I)\n",
    "\n",
    "# Load spaCy (light model for entity recognition)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "def clean_comment(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def bucket_from_p(p: float) -> str:\n",
    "    if p < 0.35:\n",
    "        return \"Negative\"\n",
    "    if p > 0.65:\n",
    "        return \"Positive\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) HF Sentiment (RoBERTa)\n",
    "# -----------------------------\n",
    "_HF_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "_tok = AutoTokenizer.from_pretrained(_HF_MODEL)\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(_HF_MODEL)\n",
    "_model.eval()\n",
    "\n",
    "def roberta_probs(text: str) -> dict:\n",
    "    if not text:\n",
    "        return {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0}\n",
    "    with torch.no_grad():\n",
    "        inputs = _tok(text[:512], return_tensors=\"pt\")\n",
    "        logits = _model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return {\"neg\": float(probs[0]), \"neu\": float(probs[1]), \"pos\": float(probs[2])}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LLM Sentiment (Gemma 2B)\n",
    "# -----------------------------\n",
    "def gemma_score_01(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.5\n",
    "    prompt = f\"\"\"\n",
    "    You are a neutral linguistic expert analyzing sentiment.\n",
    "    Evaluate only the tone of the YouTube comment.\n",
    "    Consider sarcasm and negation carefully.\n",
    "    Return JSON only: {{\"score\": <float between -1.0 and 1.0>}}\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = ollama.chat(model=\"gemma:2b\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        j = json.loads(resp[\"message\"][\"content\"])\n",
    "        s = float(j.get(\"score\", 0.0))\n",
    "    except Exception:\n",
    "        s = 0.0\n",
    "\n",
    "    p = (s + 1.0) / 2.0\n",
    "    if NEG_RE.search(text) and p > 0.5:\n",
    "        p -= 0.25\n",
    "    if SHORT_PRAISE_RE.match(text) and p > 0.7:\n",
    "        p = 0.6\n",
    "    return float(np.clip(p, 0.0, 1.0))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Ensemble Sentiment\n",
    "# -----------------------------\n",
    "def ensemble_sentiment(text: str) -> dict:\n",
    "    text_clean = clean_comment(text.lower())\n",
    "    if not text_clean:\n",
    "        return {\"p_pos\": 0.5, \"bucket\": \"Neutral\", \"p_pos_llm\": 0.5, \"p_pos_hf\": 0.33, \"p_neg_hf\": 0.33}\n",
    "\n",
    "    p_pos_llm = gemma_score_01(text_clean)\n",
    "    hf = roberta_probs(text_clean)\n",
    "    p_pos_hf, p_neg_hf = hf[\"pos\"], hf[\"neg\"]\n",
    "\n",
    "    w_llm, w_hf = 0.4, 0.6\n",
    "    p_pos = w_llm * p_pos_llm + w_hf * p_pos_hf\n",
    "\n",
    "    if p_neg_hf - p_pos_hf > 0.20 and p_pos > 0.3:\n",
    "        p_pos -= 0.20\n",
    "\n",
    "    p_pos = float(np.clip(p_pos, 0.0, 1.0))\n",
    "    bucket = bucket_from_p(p_pos)\n",
    "    return {\n",
    "        \"p_pos\": p_pos,\n",
    "        \"bucket\": bucket,\n",
    "        \"p_pos_llm\": p_pos_llm,\n",
    "        \"p_pos_hf\": p_pos_hf,\n",
    "        \"p_neg_hf\": p_neg_hf,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Likes Weighting\n",
    "# -----------------------------\n",
    "def like_weight(likes: float) -> float:\n",
    "    if likes is None or likes <= 0:\n",
    "        return 1.0\n",
    "    if likes < 10:\n",
    "        return 1.2\n",
    "    if likes < 100:\n",
    "        return 2.0\n",
    "    if likes < 500:\n",
    "        return 3.0\n",
    "    return 4.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Improved Guest Extraction\n",
    "# -----------------------------\n",
    "\n",
    "BANNED_GUEST_WORDS = {\n",
    "    \"jesus\", \"praise jesus\", \"ohhhh\", \"video\", \"motivation\", \"johari\", \"topic\", \"content\"\n",
    "}\n",
    "\n",
    "GUEST_HINTS = re.compile(r\"(with|feat\\.|featuring|guest|bring back|have .* on|invite|episode with)\", re.I)\n",
    "TITLE_CUE_RE = re.compile(r\"feat\\.|featuring|with|guest|w/\", re.I)\n",
    "\n",
    "def extract_guests(text: str, video_title: str = \"\") -> list:\n",
    "    \"\"\"\n",
    "    Extract guest names from comment or title with 3-layer logic:\n",
    "    1. spaCy PERSON entities (fast)\n",
    "    2. Gemma (if hints present)\n",
    "    3. Fallback to video title if contains 'feat.' etc.\n",
    "    \"\"\"\n",
    "    text = clean_comment(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    names = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "    # If found via spaCy, trust them (light clean)\n",
    "    if names:\n",
    "        names = [n for n in names if len(n) > 2 and n.lower() not in BANNED_GUEST_WORDS]\n",
    "        return list(set(names))\n",
    "\n",
    "    # Only call LLM if hints present\n",
    "    if not GUEST_HINTS.search(text):\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract only the names of people explicitly mentioned or suggested as podcast guests.\n",
    "    Ignore general words, religious figures, or vague text.\n",
    "    Return only JSON array of proper names (no duplicates, no empty values).\n",
    "    Example:\n",
    "    [\"Jordan Peterson\", \"Dr K\", \"Alex O'Connor\"]\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = ollama.chat(model=\"gemma:2b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        names = json.loads(resp[\"message\"][\"content\"])\n",
    "    except Exception:\n",
    "        names = []\n",
    "\n",
    "    # Post-filter\n",
    "    if isinstance(names, list):\n",
    "        names = [\n",
    "            n.strip() for n in names\n",
    "            if n and len(n) > 2 and n[0].isupper() and n.lower() not in BANNED_GUEST_WORDS\n",
    "        ]\n",
    "    else:\n",
    "        names = []\n",
    "\n",
    "    # Fallback from video title\n",
    "    if not names and TITLE_CUE_RE.search(video_title):\n",
    "        doc_t = nlp(video_title)\n",
    "        title_names = [ent.text.strip() for ent in doc_t.ents if ent.label_ == \"PERSON\"]\n",
    "        names.extend(title_names)\n",
    "\n",
    "    return list(set(names))\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Topic Request Extraction\n",
    "# -----------------------------\n",
    "TOPIC_HINTS = re.compile(r\"(talk about|episode on|discuss|cover|would love|should do|next guest|topic|content about)\", re.I)\n",
    "\n",
    "def extract_topics(text: str) -> list:\n",
    "    text = clean_comment(text)\n",
    "    if not text or not TOPIC_HINTS.search(text):\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract specific topic requests or subjects mentioned in this YouTube comment.\n",
    "    Return only JSON list of short topics.\n",
    "    Example: [\"AI\", \"mental health\", \"fitness\"]\n",
    "    Comment: \"{text}\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = ollama.chat(model=\"gemma:2b\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        topics = json.loads(resp[\"message\"][\"content\"])\n",
    "        if isinstance(topics, list):\n",
    "            return topics\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Apply All to DataFrame\n",
    "# -----------------------------\n",
    "def apply_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"cleaned_text\"] = df[\"cleaned_text\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Sentiment\n",
    "    res = df[\"cleaned_text\"].progress_apply(ensemble_sentiment)\n",
    "    df[\"sentiment_p_pos\"] = res.apply(lambda r: r[\"p_pos\"])\n",
    "    df[\"sentiment_bucket\"] = res.apply(lambda r: r[\"bucket\"])\n",
    "    df[\"p_pos_llm\"] = res.apply(lambda r: r[\"p_pos_llm\"])\n",
    "    df[\"p_pos_hf\"] = res.apply(lambda r: r[\"p_pos_hf\"])\n",
    "    df[\"p_neg_hf\"] = res.apply(lambda r: r[\"p_neg_hf\"])\n",
    "\n",
    "    # Weights\n",
    "    df[\"comment_like_count\"] = df[\"comment_like_count\"].fillna(0).astype(int)\n",
    "    df[\"comment_weight\"] = df[\"comment_like_count\"].apply(like_weight)\n",
    "    df[\"impact_weighted_sentiment\"] = df[\"sentiment_p_pos\"] * df[\"comment_weight\"]\n",
    "\n",
    "    # New columns â€” guests & topics\n",
    "    df[\"guest_mentions\"] = df.progress_apply(\n",
    "        lambda r: extract_guests(r[\"cleaned_text\"], r.get(\"video_title\", \"\")), axis=1\n",
    "    )\n",
    "\n",
    "    df[\"topic_requests\"] = df[\"cleaned_text\"].progress_apply(extract_topics)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac05a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:51<00:00,  1.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:22<00:00,  4.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Run\n",
    "# -----------------------------\n",
    "df_sent = apply_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "500064be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment_p_pos</th>\n",
       "      <th>sentiment_bucket</th>\n",
       "      <th>p_pos_llm</th>\n",
       "      <th>p_pos_hf</th>\n",
       "      <th>p_neg_hf</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_weight</th>\n",
       "      <th>impact_weighted_sentiment</th>\n",
       "      <th>guest_list</th>\n",
       "      <th>topic_requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The idea that our culture prioritizes freedom ...</td>\n",
       "      <td>the idea that our culture prioritizes freedom ...</td>\n",
       "      <td>0.152635</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>0.625315</td>\n",
       "      <td>2508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.610539</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iâ€™ve read almost every book on making money bu...</td>\n",
       "      <td>ive read almost every book on making money but...</td>\n",
       "      <td>0.808495</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.724158</td>\n",
       "      <td>0.047799</td>\n",
       "      <td>1191</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.233979</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He who knows that enough is enough will always...</td>\n",
       "      <td>he who knows that enough is enough will always...</td>\n",
       "      <td>0.630817</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.509695</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>1008</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.523268</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iâ€™m a doctor and â€˜Secrets To Perfect Healthâ€™ b...</td>\n",
       "      <td>im a doctor and secrets to perfect health by n...</td>\n",
       "      <td>0.902741</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.944569</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>801</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.610966</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The world is a scam ðŸ˜‚</td>\n",
       "      <td>the world is a scam</td>\n",
       "      <td>0.208264</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.877446</td>\n",
       "      <td>603</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.833058</td>\n",
       "      <td>[Morgan Housel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  The idea that our culture prioritizes freedom ...   \n",
       "1  Iâ€™ve read almost every book on making money bu...   \n",
       "2  He who knows that enough is enough will always...   \n",
       "3  Iâ€™m a doctor and â€˜Secrets To Perfect Healthâ€™ b...   \n",
       "4                              The world is a scam ðŸ˜‚   \n",
       "\n",
       "                                        cleaned_text  sentiment_p_pos  \\\n",
       "0  the idea that our culture prioritizes freedom ...         0.152635   \n",
       "1  ive read almost every book on making money but...         0.808495   \n",
       "2  he who knows that enough is enough will always...         0.630817   \n",
       "3  im a doctor and secrets to perfect health by n...         0.902741   \n",
       "4                                the world is a scam         0.208264   \n",
       "\n",
       "  sentiment_bucket  p_pos_llm  p_pos_hf  p_neg_hf  comment_like_count  \\\n",
       "0         Negative     0.8350  0.031058  0.625315                2508   \n",
       "1         Positive     0.9350  0.724158  0.047799                1191   \n",
       "2          Neutral     0.8125  0.509695  0.024620                1008   \n",
       "3         Positive     0.8400  0.944569  0.008702                 801   \n",
       "4         Negative     0.5000  0.013774  0.877446                 603   \n",
       "\n",
       "   comment_weight  impact_weighted_sentiment       guest_list topic_requests  \n",
       "0             4.0                   0.610539  [Morgan Housel]             []  \n",
       "1             4.0                   3.233979  [Morgan Housel]             []  \n",
       "2             4.0                   2.523268  [Morgan Housel]             []  \n",
       "3             4.0                   3.610966  [Morgan Housel]             []  \n",
       "4             4.0                   0.833058  [Morgan Housel]             []  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent[[\n",
    "    \"comment_text\",\n",
    "    \"cleaned_text\",\n",
    "    \"sentiment_p_pos\",\n",
    "    \"sentiment_bucket\",\n",
    "    \"p_pos_llm\",\n",
    "    \"p_pos_hf\",\n",
    "    \"p_neg_hf\",\n",
    "    \"comment_like_count\",\n",
    "    \"comment_weight\",\n",
    "    \"impact_weighted_sentiment\",\n",
    "    \"guest_list\",\n",
    "    \"topic_requests\"\n",
    "]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76d6f3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_bucket\n",
       "Negative    42\n",
       "Positive    37\n",
       "Neutral     21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['sentiment_bucket'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec65355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guest_list\n",
       "[Morgan Housel]    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['guest_list'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3946ac6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_requests\n",
       "[]    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['topic_requests'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41fdd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sent.to_csv('/Users/riadanas/Desktop/MLE Diary of a CEO/data/processed/processed_snapshot4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6a907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-diary-of-a-ceo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
