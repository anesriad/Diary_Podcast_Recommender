{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a424fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/riadanas/Desktop/steven_bartlett_project/data/raw/DIARY_all_pod.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1fbcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(df.shape)\\n\\n# Keep a small sample for testing\\ndf = df.head(500)\\n\\n\\nprint(df.shape)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(df.shape)\n",
    "\n",
    "# Keep a small sample for testing\n",
    "df = df.head(500)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4de543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402054, 18)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccf9ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 18)\n",
      "video_id\n",
      "NyFSkGMWP5Q    38\n",
      "ldizQkuWpDE    22\n",
      "Hik6OY-nk4c    15\n",
      "atejm2w2jWY     7\n",
      "It5_C6AF1pk     7\n",
      "ZxXruY7llcc     6\n",
      "EdlXcVu1CTs     3\n",
      "0GQozcTPyO0     2\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!                          38\n",
      "Body Language Expert Explains Why People Dislike You                                                  22\n",
      "Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!           15\n",
      "Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!     7\n",
      "Exercise & Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!     7\n",
      "Peter Attia: Anti-aging Cure No One Talks About! 50% Chance You’ll Die In A Year If This Happens!      6\n",
      "The Business Expert: How To Build A Brand In 2025! They're Lying To You About Work-Life Balance!       3\n",
      "Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.       2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77400</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>NyFSkGMWP5Q</td>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip ...</td>\n",
       "      <td>88% of adults have metabolic disease, but what...</td>\n",
       "      <td>2025-02-06T08:00:17Z</td>\n",
       "      <td>5387358</td>\n",
       "      <td>162020</td>\n",
       "      <td>10597</td>\n",
       "      <td>UgyU9rxYG4P0NUL8WA54AaABAg.AJJ7oTBGcPJAM8MkWBCtqL</td>\n",
       "      <td>Great to hear about your turn around.   Bravo!</td>\n",
       "      <td>@roflorida1</td>\n",
       "      <td>UC7q8r-7ahW4TtkoUziiQoww</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-08-22T19:50:30Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>UgyU9rxYG4P0NUL8WA54AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58070</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>Hik6OY-nk4c</td>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your ...</td>\n",
       "      <td>Dr Jordan Peterson is a world-renowned former ...</td>\n",
       "      <td>2025-01-13T08:00:19Z</td>\n",
       "      <td>2046763</td>\n",
       "      <td>54184</td>\n",
       "      <td>6392</td>\n",
       "      <td>UgyXAbAEOln-oBI6w4h4AaABAg.ADEVDGzRuVDADEiCiG23KF</td>\n",
       "      <td>Not a very open minded view....</td>\n",
       "      <td>@NiniM8154</td>\n",
       "      <td>UCLyDLLBemSNUmPREQpFGr5A</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-13T14:19:37Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>UgyXAbAEOln-oBI6w4h4AaABAg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             channel_name                channel_id     video_id  \\\n",
       "77400  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  NyFSkGMWP5Q   \n",
       "58070  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  Hik6OY-nk4c   \n",
       "\n",
       "                                             video_title  \\\n",
       "77400  The Insulin & Glucose Doctor: This Will Strip ...   \n",
       "58070  Jordan B Peterson: You Need To Listen To Your ...   \n",
       "\n",
       "                                       video_description  \\\n",
       "77400  88% of adults have metabolic disease, but what...   \n",
       "58070  Dr Jordan Peterson is a world-renowned former ...   \n",
       "\n",
       "         video_published_at  view_count  video_like_count  comment_count  \\\n",
       "77400  2025-02-06T08:00:17Z     5387358            162020          10597   \n",
       "58070  2025-01-13T08:00:19Z     2046763             54184           6392   \n",
       "\n",
       "                                              comment_id  \\\n",
       "77400  UgyU9rxYG4P0NUL8WA54AaABAg.AJJ7oTBGcPJAM8MkWBCtqL   \n",
       "58070  UgyXAbAEOln-oBI6w4h4AaABAg.ADEVDGzRuVDADEiCiG23KF   \n",
       "\n",
       "                                         comment_text       author  \\\n",
       "77400  Great to hear about your turn around.   Bravo!  @roflorida1   \n",
       "58070                 Not a very open minded view....   @NiniM8154   \n",
       "\n",
       "                      author_id  comment_like_count  comment_published_at  \\\n",
       "77400  UC7q8r-7ahW4TtkoUziiQoww                   0  2025-08-22T19:50:30Z   \n",
       "58070  UCLyDLLBemSNUmPREQpFGr5A                   1  2025-01-13T14:19:37Z   \n",
       "\n",
       "       is_pinned  is_reply           parent_comment_id  \n",
       "77400      False      True  UgyU9rxYG4P0NUL8WA54AaABAg  \n",
       "58070      False      True  UgyXAbAEOln-oBI6w4h4AaABAg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### If I want to pick random samples from a few videos\n",
    "import random\n",
    "\n",
    "video_ids = df['video_id'].unique()[5:13]\n",
    "df = df[df['video_id'].isin(video_ids)].sample(n=100)\n",
    "\n",
    "print(df.shape)\n",
    "print(df['video_id'].value_counts())\n",
    "print(df['video_title'].value_counts())\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86197c14",
   "metadata": {},
   "source": [
    "## Guest Name Processing - GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98549441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_guest_names_openrouter(description: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract true podcast guest names from a YouTube description using OpenRouter (Claude 3.5 / GPT-4-mini).\n",
    "    Ignores names used as references or examples.\n",
    "    \"\"\"\n",
    "    if not isinstance(description, str) or not description.strip():\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a podcast metadata assistant.\n",
    "\n",
    "    Task:\n",
    "    - Read the YouTube video description carefully.\n",
    "    - Identify ONLY the actual guest(s) who appear in the episode or are directly interviewed.\n",
    "    - If a guest's name was misspelled, correct it based on context.\n",
    "    - Make sure to not miss guests that go by nicknames (e.g., \"The Rock\" or \"MrBeast\").\n",
    "    - Ignore people mentioned just as examples, comparisons, or references (e.g., Warren Buffett, Elon Musk) unless they are clearly stated as guests.\n",
    "    - If multiple guests appear, include all of them.\n",
    "    - Preserve professional titles (e.g., \"Dr\", \"Prof\", \"Sir\") if present.\n",
    "    - Return a clean JSON list of guest names, for example:\n",
    "      [\"Morgan Housel\"]\n",
    "      or [\"Dr Andrew Huberman\", \"Lex Fridman\"]\n",
    "    - If no guest is clearly identified, return an empty list [].\n",
    "\n",
    "    Description:\n",
    "    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"openai/gpt-4o-mini\",  # you can change to \"openai/gpt-4o-mini\"\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "\n",
    "        content = completion.choices[0].message.content.strip()\n",
    "\n",
    "        # Try parsing JSON\n",
    "        try:\n",
    "            result = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r'\\[(.*?)\\]', content)\n",
    "            if match:\n",
    "                inner = match.group(1)\n",
    "                result = [n.strip().strip('\"').strip() for n in inner.split(\",\") if n.strip()]\n",
    "            else:\n",
    "                result = re.findall(r\"(?:Dr\\.?|Prof\\.?|Mr\\.?|Ms\\.?)?\\s?[A-Z][a-z]+(?:\\s[A-Z][a-z]+)+\", content)\n",
    "\n",
    "        if isinstance(result, str):\n",
    "            result = [result]\n",
    "        result = [r.strip() for r in result if r.strip()]\n",
    "        result = list(set(result))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing description: {e}\")\n",
    "        result = []\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 🔁 Apply once per unique video_id\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def assign_guest_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply guest extraction once per unique video_id.\n",
    "    Adds a 'guest_list' column to the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create mapping: video_id → guest list\n",
    "    mapping = {}\n",
    "    unique_videos = df.drop_duplicates(subset=\"video_id\")[[\"video_id\", \"video_description\"]]\n",
    "\n",
    "    for _, row in unique_videos.iterrows():\n",
    "        vid = row[\"video_id\"]\n",
    "        desc = row[\"video_description\"]\n",
    "        guests = get_guest_names_openrouter(desc)\n",
    "        mapping[vid] = guests\n",
    "\n",
    "    # Map results back to main DataFrame\n",
    "    df[\"guest_list\"] = df[\"video_id\"].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa58b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assign_guest_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2eadca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guest_list\n",
       "[Dr Benjamin Bikman]             38\n",
       "[Steven, Vanessa Van Edwards]    22\n",
       "[Dr Jordan Peterson]             15\n",
       "[Master Shi Heng Yi]              7\n",
       "[Dr Stacy Sims]                   7\n",
       "[Dr Peter Attia]                  6\n",
       "[Emma Grede]                      3\n",
       "[Dr. Sara Szal]                   2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['guest_list'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f59ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_published_at</th>\n",
       "      <th>is_pinned</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>guest_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77400</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>NyFSkGMWP5Q</td>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip ...</td>\n",
       "      <td>88% of adults have metabolic disease, but what...</td>\n",
       "      <td>2025-02-06T08:00:17Z</td>\n",
       "      <td>5387358</td>\n",
       "      <td>162020</td>\n",
       "      <td>10597</td>\n",
       "      <td>UgyU9rxYG4P0NUL8WA54AaABAg.AJJ7oTBGcPJAM8MkWBCtqL</td>\n",
       "      <td>Great to hear about your turn around.   Bravo!</td>\n",
       "      <td>@roflorida1</td>\n",
       "      <td>UC7q8r-7ahW4TtkoUziiQoww</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-08-22T19:50:30Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>UgyU9rxYG4P0NUL8WA54AaABAg</td>\n",
       "      <td>[Dr Benjamin Bikman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58070</th>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>Hik6OY-nk4c</td>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your ...</td>\n",
       "      <td>Dr Jordan Peterson is a world-renowned former ...</td>\n",
       "      <td>2025-01-13T08:00:19Z</td>\n",
       "      <td>2046763</td>\n",
       "      <td>54184</td>\n",
       "      <td>6392</td>\n",
       "      <td>UgyXAbAEOln-oBI6w4h4AaABAg.ADEVDGzRuVDADEiCiG23KF</td>\n",
       "      <td>Not a very open minded view....</td>\n",
       "      <td>@NiniM8154</td>\n",
       "      <td>UCLyDLLBemSNUmPREQpFGr5A</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-13T14:19:37Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>UgyXAbAEOln-oBI6w4h4AaABAg</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             channel_name                channel_id     video_id  \\\n",
       "77400  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  NyFSkGMWP5Q   \n",
       "58070  The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw  Hik6OY-nk4c   \n",
       "\n",
       "                                             video_title  \\\n",
       "77400  The Insulin & Glucose Doctor: This Will Strip ...   \n",
       "58070  Jordan B Peterson: You Need To Listen To Your ...   \n",
       "\n",
       "                                       video_description  \\\n",
       "77400  88% of adults have metabolic disease, but what...   \n",
       "58070  Dr Jordan Peterson is a world-renowned former ...   \n",
       "\n",
       "         video_published_at  view_count  video_like_count  comment_count  \\\n",
       "77400  2025-02-06T08:00:17Z     5387358            162020          10597   \n",
       "58070  2025-01-13T08:00:19Z     2046763             54184           6392   \n",
       "\n",
       "                                              comment_id  \\\n",
       "77400  UgyU9rxYG4P0NUL8WA54AaABAg.AJJ7oTBGcPJAM8MkWBCtqL   \n",
       "58070  UgyXAbAEOln-oBI6w4h4AaABAg.ADEVDGzRuVDADEiCiG23KF   \n",
       "\n",
       "                                         comment_text       author  \\\n",
       "77400  Great to hear about your turn around.   Bravo!  @roflorida1   \n",
       "58070                 Not a very open minded view....   @NiniM8154   \n",
       "\n",
       "                      author_id  comment_like_count  comment_published_at  \\\n",
       "77400  UC7q8r-7ahW4TtkoUziiQoww                   0  2025-08-22T19:50:30Z   \n",
       "58070  UCLyDLLBemSNUmPREQpFGr5A                   1  2025-01-13T14:19:37Z   \n",
       "\n",
       "       is_pinned  is_reply           parent_comment_id            guest_list  \n",
       "77400      False      True  UgyU9rxYG4P0NUL8WA54AaABAg  [Dr Benjamin Bikman]  \n",
       "58070      False      True  UgyXAbAEOln-oBI6w4h4AaABAg  [Dr Jordan Peterson]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc1c7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'88% of adults have metabolic disease, but what’s really making us sick? Dr Benjamin Bikman reveals the hidden dangers of insulin and how to take control of your health  \\xa0Dr Benjamin Bikman is a metabolic scientist and Professor of Cell Biology and Physiology. He is the host of ‘The Metabolic Classroom’ podcast and author of books such as, ‘Why We Get Sick: The Hidden Epidemic at the Root of Most Chronic Disease―and How to Fight It’.\\xa0  00:00 Intro 02:19 My Mission to Help with Chronic Diseases 05'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['video_description'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f654891",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91a345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Text Cleaning (light only)\n",
    "# ------------------------------------------------------\n",
    "def clean_comment(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning for comments:\n",
    "    - Remove @mentions\n",
    "    - Remove URLs\n",
    "    - Remove emojis / non-ascii\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    - Keep context words (no lemmatization, no stopword removal yet)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # remove mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    # remove emojis/non-ascii\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # lowercase + strip\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6529d3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77400</th>\n",
       "      <td>Great to hear about your turn around.   Bravo!</td>\n",
       "      <td>great to hear about your turn around.   bravo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58070</th>\n",
       "      <td>Not a very open minded view....</td>\n",
       "      <td>not a very open minded view....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49748</th>\n",
       "      <td>In many countries 😢</td>\n",
       "      <td>in many countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82498</th>\n",
       "      <td>1 hour and 43 minutes. He refferences biggest loser and he says they gain it all back. Which is ...</td>\n",
       "      <td>1 hour and 43 minutes. he refferences biggest loser and he says they gain it all back. which is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75284</th>\n",
       "      <td>This channel needs to catch up with the world and what matters the most!</td>\n",
       "      <td>this channel needs to catch up with the world and what matters the most!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65006</th>\n",
       "      <td>I'm happy for you \\n💕❤️💕❤️💕❤️💕\\n✅✅✅✅✅✅✅\\n😍😍😍😍😍😍😍</td>\n",
       "      <td>i'm happy for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47985</th>\n",
       "      <td>Peter said 15% die within a year - not 50%.  Click bait?</td>\n",
       "      <td>peter said 15% die within a year - not 50%.  click bait?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78134</th>\n",
       "      <td>Is that for us to answer? If you had to ask and it's for you, I guess you know the answer 🤦🏾‍♀️</td>\n",
       "      <td>is that for us to answer? if you had to ask and it's for you, i guess you know the answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51625</th>\n",
       "      <td>Literally proves her point about an employee mindset....</td>\n",
       "      <td>literally proves her point about an employee mindset....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77701</th>\n",
       "      <td>the brain needs cholesterol. \\ngreat guest. ty</td>\n",
       "      <td>the brain needs cholesterol. \\ngreat guest. ty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              comment_text  \\\n",
       "77400                                                       Great to hear about your turn around.   Bravo!   \n",
       "58070                                                                      Not a very open minded view....   \n",
       "49748                                                                                  In many countries 😢   \n",
       "82498  1 hour and 43 minutes. He refferences biggest loser and he says they gain it all back. Which is ...   \n",
       "75284                             This channel needs to catch up with the world and what matters the most!   \n",
       "65006                                                     I'm happy for you \\n💕❤️💕❤️💕❤️💕\\n✅✅✅✅✅✅✅\\n😍😍😍😍😍😍😍   \n",
       "47985                                             Peter said 15% die within a year - not 50%.  Click bait?   \n",
       "78134      Is that for us to answer? If you had to ask and it's for you, I guess you know the answer 🤦🏾‍♀️   \n",
       "51625                                             Literally proves her point about an employee mindset....   \n",
       "77701                                                       the brain needs cholesterol. \\ngreat guest. ty   \n",
       "\n",
       "                                                                                              cleaned_text  \n",
       "77400                                                       great to hear about your turn around.   bravo!  \n",
       "58070                                                                      not a very open minded view....  \n",
       "49748                                                                                    in many countries  \n",
       "82498  1 hour and 43 minutes. he refferences biggest loser and he says they gain it all back. which is ...  \n",
       "75284                             this channel needs to catch up with the world and what matters the most!  \n",
       "65006                                                                                    i'm happy for you  \n",
       "47985                                             peter said 15% die within a year - not 50%.  click bait?  \n",
       "78134            is that for us to answer? if you had to ask and it's for you, i guess you know the answer  \n",
       "51625                                             literally proves her point about an employee mindset....  \n",
       "77701                                                       the brain needs cholesterol. \\ngreat guest. ty  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply light cleaning to the comment_text column\n",
    "df[\"cleaned_text\"] = df[\"comment_text\"].apply(clean_comment)\n",
    "\n",
    "# Preview the results\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df[[\"comment_text\", \"cleaned_text\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb6f9e",
   "metadata": {},
   "source": [
    "## Topic category (llama) -> Once per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5bc5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "def get_topic_category(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Ollama to classify the video title into a topic category.\n",
    "    Example categories: health, mental health, productivity, finance, relationships, entrepreneurship, other.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Categorize the following YouTube video title into ONE broad category:\n",
    "    - health\n",
    "    - mental health / psychology\n",
    "    - productivity / personal development\n",
    "    - finance\n",
    "    - relationships\n",
    "    - entrepreneurship / business\n",
    "    - Religion / Spirituality\n",
    "    - Technology\n",
    "    - Education\n",
    "    - Lifestyle\n",
    "    - Entertainment\n",
    "    - other\n",
    "\n",
    "    Title: \"{title}\"\n",
    "\n",
    "    Return only the category name, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2:3b\",  # you can swap to another local model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30034ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Extract unique video_id/title pairs\n",
    "video_meta = df[[\"video_id\", \"video_title\"]].drop_duplicates()\n",
    "\n",
    "# Apply Ollama category classification\n",
    "video_meta[\"Topic_Category\"] = video_meta[\"video_title\"].apply(get_topic_category)\n",
    "\n",
    "# Merge back into main dataframe\n",
    "df = df.merge(video_meta[[\"video_id\", \"Topic_Category\"]], on=\"video_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aec2a9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic_Category\n",
       "health                                 53\n",
       "personal development                   22\n",
       "mental health / psychology             15\n",
       "other                                   7\n",
       "productivity / personal development     3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cb8a7",
   "metadata": {},
   "source": [
    "## Sentiment/Comment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0083932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Fast, Scalable Comment NLP\n",
    "# ===========================\n",
    "\n",
    "# - Batched RoBERTa inference (vectorized on CPU/GPU)\n",
    "# - Parallel LLM calls (Gemma) for sentiment + guest/topic when used\n",
    "# - Optional SQLite caching (sentiment & meta) to skip repeat work\n",
    "# - Batched spaCy with nlp.pipe for names in comments/titles\n",
    "#\n",
    "# - We still call the LLM for sentiment on EVERY row (no gating)\n",
    "# - Guest logic: PERSON anywhere in text -> accept; else LLM only if hints; else title fallback\n",
    "# - Topic logic: only if topic-hint -> LLM extraction (no proximity filters)\n",
    "# - Same ensemble weights and heuristics\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "import ollama\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Helpers & resources\n",
    "# -----------------------------\n",
    "NEG_PHRASES = [\n",
    "    r\"\\bnot good\\b\", r\"\\bnot great\\b\", r\"\\bnot helpful\\b\", r\"\\bdon't like\\b\", r\"\\bdont like\\b\",\n",
    "    r\"\\bnot worth\\b\", r\"\\bwaste of time\\b\", r\"\\btoo long\\b\", r\"\\btoo slow\\b\", r\"\\btoo loud\\b\",\n",
    "    r\"\\bmisleading\\b\", r\"\\bclickbait\\b\", r\"\\bbiased\\b\", r\"\\bconfusing\\b\", r\"\\bannoying\\b\",\n",
    "    r\"\\bcringe\\b\", r\"\\bstupid\\b\", r\"\\bdumb\\b\", r\"\\bbad\\b\", r\"\\bawful\\b\", r\"\\bterrible\\b\",\n",
    "    r\"\\buseless\\b\", r\"\\bpointless\\b\", r\"\\bwrong\\b\", r\"\\bpoor (audio|sound|quality)\\b\",\n",
    "    r\"\\birrelevant\\b\", r\"\\boff\\-topic\\b\", r\"\\bproblem\\b\", r\"\\bissue\\b\", r\"\\bdisappoint(ing|ed)\\b\",\n",
    "    r\"\\brude\\b\", r\"\\boffensive\\b\", r\"\\bunfunny\\b\", r\"\\bboring\\b\", r\"\\blazy\\b\", r\"\\btoxic\\b\",\n",
    "    r\"\\bhate\\b\", r\"\\bgarbage\\b\", r\"\\bignorant\\b\", r\"\\bweird\\b\", r\"\\bnegative\\b\", r\"\\bbroken\\b\",\n",
    "    r\"\\bdownvote\\b\", r\"\\bterribly\\b\", r\"\\bdislike\\b\", r\"\\bpathetic\\b\", r\"\\bworse\\b\"\n",
    "]\n",
    "NEG_RE = re.compile(\"|\".join(NEG_PHRASES), re.I)\n",
    "SHORT_PRAISE_RE = re.compile(r\"^(nice|cool|great|good|amazing|awesome|wow|love|thanks|perfect)[.!]?$\", re.I)\n",
    "\n",
    "# Hints for guest & topic LLM calls (same as Code 1)\n",
    "GUEST_HINTS = re.compile(r\"(with|feat\\.|featuring|guest|bring back|have .* on|invite|episode with|talk to|feature)\", re.I)\n",
    "TITLE_CUE_RE = re.compile(r\"feat\\.|featuring|with|guest|w/\", re.I)\n",
    "TOPIC_HINTS = re.compile(r\"(talk about|discuss|more on|more of|do one about|episode on|video on|cover|speak about|conversation on|should do|would love|should talk about|content about)\", re.I)\n",
    "\n",
    "# Light stoplist for junky \"names\" (same spirit as Code 1)\n",
    "BANNED_GUEST_WORDS = {\n",
    "    \"jesus\", \"praise jesus\", \"ohhhh\", \"video\", \"motivation\", \"johari\", \"topic\", \"content\",\n",
    "    \"someone\", \"somebody\", \"anyone\", \"everybody\"\n",
    "}\n",
    "\n",
    "# spaCy NER (as in Code 1: we only need NER; parser/tagger/lemmatizer disabled)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "def clean_comment(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text.strip()\n",
    "\n",
    "def bucket_from_p(p: float) -> str:\n",
    "    if p < 0.35:\n",
    "        return \"Negative\"\n",
    "    if p > 0.65:\n",
    "        return \"Positive\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def like_weight(likes: float) -> float:\n",
    "    if likes is None or likes <= 0:\n",
    "        return 1.0\n",
    "    if likes < 10:\n",
    "        return 1.2\n",
    "    if likes < 100:\n",
    "        return 2.0\n",
    "    if likes < 500:\n",
    "        return 3.0\n",
    "    return 4.0\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) HF Sentiment (RoBERTa) — BATCHED for speed (same model/logic)\n",
    "# -----------------------------\n",
    "_HF_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "_tok = AutoTokenizer.from_pretrained(_HF_MODEL)\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(_HF_MODEL)\n",
    "_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "_model.to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def roberta_probs_batch(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized version of Code 1's single-text call.\n",
    "    - We still trim very long strings (char-level) to ~512 to mimic Code 1's behavior.\n",
    "    - We enable truncation to model_max_length for safety when token count overruns.\n",
    "    Returns array shape (N, 3) = [neg, neu, pos] per row.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return np.zeros((0, 3), dtype=float)\n",
    "    chopped = [(t[:512] if isinstance(t, str) else \"\") for t in texts]\n",
    "    probs_all = []\n",
    "    bs = 256  # tune as needed\n",
    "    for i in range(0, len(chopped), bs):\n",
    "        batch = chopped[i:i + bs]\n",
    "        enc = _tok(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=min(512, _tok.model_max_length),\n",
    "        )\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        logits = _model(**enc).logits\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "    return np.vstack(probs_all)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LLM Sentiment (Gemma 2B) — PARALLEL for speed (same prompt/heuristics)\n",
    "# -----------------------------\n",
    "def _gemma_sent_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a neutral linguistic expert analyzing sentiment.\n",
    "Evaluate only the tone of the YouTube comment.\n",
    "Consider sarcasm and negation carefully.\n",
    "Return JSON only: {{\"score\": <float between -1.0 and 1.0>}}\n",
    "Comment: \"{text}\"\n",
    "\"\"\".strip()\n",
    "\n",
    "def gemma_score_01(text: str, model_name: str = \"gemma:2b\") -> float:\n",
    "    if not text:\n",
    "        return 0.5\n",
    "    try:\n",
    "        resp = ollama.chat(model=model_name, messages=[{\"role\":\"user\",\"content\":_gemma_sent_prompt(text)}])\n",
    "        j = json.loads(resp[\"message\"][\"content\"])\n",
    "        s = float(j.get(\"score\", 0.0))\n",
    "    except Exception:\n",
    "        s = 0.0\n",
    "\n",
    "    p = (s + 1.0) / 2.0\n",
    "    if NEG_RE.search(text) and p > 0.5:\n",
    "        p -= 0.25\n",
    "    if SHORT_PRAISE_RE.match(text) and p > 0.7:\n",
    "        p = 0.6\n",
    "    return float(np.clip(p, 0.0, 1.0))\n",
    "\n",
    "def parallel_llm_scores(texts: List[str], max_workers: int = 8, model_name: str = \"gemma:2b\") -> List[float]:\n",
    "    scores = [0.5] * len(texts)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(gemma_score_01, t, model_name): i for i, t in enumerate(texts)}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"LLM sentiment (parallel)\"):\n",
    "            i = futs[fut]\n",
    "            try:\n",
    "                scores[i] = fut.result()\n",
    "            except Exception:\n",
    "                scores[i] = 0.5\n",
    "    return scores\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Ensemble Sentiment — SAME MATH as Code 1, but batched\n",
    "# -----------------------------\n",
    "def ensemble_sentiment_batch(texts: List[str],\n",
    "                             llm_workers: int = 8,\n",
    "                             llm_model: str = \"gemma:2b\") -> Dict[str, np.ndarray]:\n",
    "    cleaned = [clean_comment((t or \"\").lower()) for t in texts]\n",
    "    # HF (vectorized)\n",
    "    hf_probs = roberta_probs_batch(cleaned)\n",
    "    if hf_probs.shape[0] == 0:\n",
    "        n = len(texts)\n",
    "        return dict(\n",
    "            p_pos=np.full(n, 0.5),\n",
    "            bucket=np.array([\"Neutral\"] * n, dtype=object),\n",
    "            p_pos_llm=np.full(n, 0.5),\n",
    "            p_pos_hf=np.full(n, 1/3),\n",
    "            p_neg_hf=np.full(n, 1/3),\n",
    "        )\n",
    "    p_neg_hf, p_neu_hf, p_pos_hf = hf_probs[:, 0], hf_probs[:, 1], hf_probs[:, 2]\n",
    "\n",
    "    # LLM (parallel for EVERY row — just like Code 1)\n",
    "    p_pos_llm = np.array(parallel_llm_scores(cleaned, max_workers=llm_workers, model_name=llm_model), dtype=float)\n",
    "\n",
    "    # Same blend & penalty as Code 1\n",
    "    w_llm, w_hf = 0.4, 0.6\n",
    "    p_pos = w_llm * p_pos_llm + w_hf * p_pos_hf\n",
    "    mask_penalty = (p_neg_hf - p_pos_hf) > 0.20\n",
    "    p_pos[mask_penalty] = np.maximum(0.0, p_pos[mask_penalty] - 0.20)\n",
    "    p_pos = np.clip(p_pos, 0.0, 1.0)\n",
    "\n",
    "    # Buckets (vectorized)\n",
    "    bucket = np.empty(len(p_pos), dtype=object)\n",
    "    bucket[p_pos < 0.35] = \"Negative\"\n",
    "    bucket[p_pos > 0.65] = \"Positive\"\n",
    "    bucket[(p_pos >= 0.35) & (p_pos <= 0.65)] = \"Neutral\"\n",
    "\n",
    "    return dict(\n",
    "        p_pos=p_pos,\n",
    "        bucket=bucket,\n",
    "        p_pos_llm=p_pos_llm,\n",
    "        p_pos_hf=p_pos_hf,\n",
    "        p_neg_hf=p_neg_hf,\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Caching (sentiment + meta)\n",
    "# -----------------------------\n",
    "CACHE_PATH = os.path.abspath(\"./sentiment_cache.sqlite\")\n",
    "\n",
    "def _ensure_cache():\n",
    "    con = sqlite3.connect(CACHE_PATH)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS sentiments (\n",
    "        comment_id TEXT PRIMARY KEY,\n",
    "        text_hash  TEXT NOT NULL,\n",
    "        p_pos      REAL NOT NULL,\n",
    "        bucket     TEXT NOT NULL,\n",
    "        p_pos_llm  REAL NOT NULL,\n",
    "        p_pos_hf   REAL NOT NULL,\n",
    "        p_neg_hf   REAL NOT NULL,\n",
    "        created_at TEXT NOT NULL\n",
    "      );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS meta_extractions (\n",
    "        comment_id  TEXT PRIMARY KEY,\n",
    "        text_hash   TEXT NOT NULL,\n",
    "        guest_json  TEXT NOT NULL,\n",
    "        topic_json  TEXT NOT NULL,\n",
    "        created_at  TEXT NOT NULL\n",
    "      );\n",
    "    \"\"\")\n",
    "    con.commit(); con.close()\n",
    "\n",
    "def cache_get_sentiments(comment_ids, text_hashes):\n",
    "    if not comment_ids: return {}\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    qmarks = \",\".join([\"?\"] * len(comment_ids))\n",
    "    rows = cur.execute(f\"SELECT * FROM sentiments WHERE comment_id IN ({qmarks})\", comment_ids).fetchall()\n",
    "    con.close()\n",
    "    out = {}\n",
    "    for (cid, th, p_pos, bucket, p_pos_llm, p_pos_hf, p_neg_hf, created_at) in rows:\n",
    "        if text_hashes.get(cid) == th:\n",
    "            out[cid] = dict(p_pos=p_pos, bucket=bucket, p_pos_llm=p_pos_llm, p_pos_hf=p_pos_hf, p_neg_hf=p_neg_hf)\n",
    "    return out\n",
    "\n",
    "def cache_put_sentiments(rows):\n",
    "    if not rows: return\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "      INSERT INTO sentiments (comment_id, text_hash, p_pos, bucket, p_pos_llm, p_pos_hf, p_neg_hf, created_at)\n",
    "      VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "      ON CONFLICT(comment_id) DO UPDATE SET\n",
    "        text_hash=excluded.text_hash,\n",
    "        p_pos=excluded.p_pos,\n",
    "        bucket=excluded.bucket,\n",
    "        p_pos_llm=excluded.p_pos_llm,\n",
    "        p_pos_hf=excluded.p_pos_hf,\n",
    "        p_neg_hf=excluded.p_neg_hf,\n",
    "        created_at=excluded.created_at;\n",
    "    \"\"\", [(cid, th, ppos, buck, pll, phf, nhf, datetime.utcnow().isoformat()) for (cid, th, ppos, buck, pll, phf, nhf) in rows])\n",
    "    con.commit(); con.close()\n",
    "\n",
    "def cache_get_meta(comment_ids, text_hashes):\n",
    "    if not comment_ids: return {}\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    qmarks = \",\".join([\"?\"] * len(comment_ids))\n",
    "    rows = cur.execute(f\"SELECT * FROM meta_extractions WHERE comment_id IN ({qmarks})\", comment_ids).fetchall()\n",
    "    con.close()\n",
    "    out = {}\n",
    "    for (cid, th, gj, tj, created_at) in rows:\n",
    "        if text_hashes.get(cid) == th:\n",
    "            out[cid] = {\"guest_mentions\": json.loads(gj), \"topic_requests\": json.loads(tj)}\n",
    "    return out\n",
    "\n",
    "def cache_put_meta(rows):\n",
    "    if not rows: return\n",
    "    con = sqlite3.connect(CACHE_PATH); cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "      INSERT INTO meta_extractions (comment_id, text_hash, guest_json, topic_json, created_at)\n",
    "      VALUES (?, ?, ?, ?, ?)\n",
    "      ON CONFLICT(comment_id) DO UPDATE SET\n",
    "        text_hash=excluded.text_hash,\n",
    "        guest_json=excluded.guest_json,\n",
    "        topic_json=excluded.topic_json,\n",
    "        created_at=excluded.created_at;\n",
    "    \"\"\", [(cid, th, json.dumps(guests), json.dumps(topics), datetime.utcnow().isoformat()) for (cid, th, guests, topics) in rows])\n",
    "    con.commit(); con.close()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Guest Extraction — SAME LOGIC as Code 1, but batched\n",
    "# -----------------------------\n",
    "def _llm_guest_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are analyzing a YouTube comment on a podcast episode.\n",
    "\n",
    "Your goal is to extract only the names of people the viewer *explicitly or implicitly wants to appear as guests* on future episodes.\n",
    "\n",
    "Focus on:\n",
    "- Mentions framed as requests or desires (e.g. “bring X on”, “invite Y”, “you should talk to Z”, “have A back”).\n",
    "- Include both full names and recognizable single names or titles (e.g. “Dr. Huberman”, “Peterson”, “Elon Musk”).\n",
    "- Ignore names mentioned for other reasons (examples, comparisons, stories).\n",
    "- Exclude religious figures, fictional characters, vague references, or non-human entities.\n",
    "\n",
    "Return strictly a JSON array of names (no explanation, no duplicates, no empty entries).\n",
    "\n",
    "Example:\n",
    "[\"Jordan Peterson\", \"Dr. Rhonda Patrick\", \"Elon Musk\"]\n",
    "\n",
    "Comment: \"{text}\"\n",
    "\"\"\".strip()\n",
    "\n",
    "def _llm_extract_guests_single(text: str, model_name: str = \"gemma:2b\") -> List[str]:\n",
    "    try:\n",
    "        resp = ollama.chat(model=model_name, messages=[{\"role\":\"user\",\"content\":_llm_guest_prompt(text)}])\n",
    "        names = json.loads(resp[\"message\"][\"content\"])\n",
    "    except Exception:\n",
    "        names = []\n",
    "\n",
    "    if isinstance(names, list):\n",
    "        names = [\n",
    "            n.strip() for n in names\n",
    "            if n and len(n) > 2 and n[0].isupper() and n.lower() not in BANNED_GUEST_WORDS\n",
    "        ]\n",
    "    else:\n",
    "        names = []\n",
    "    return list(dict.fromkeys(names))  # dedupe, keep order\n",
    "\n",
    "def extract_guests_batch(texts: List[str], titles: Optional[List[str]] = None,\n",
    "                         max_workers: int = 8, llm_model: str = \"gemma:2b\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Code 1 flow, just faster:\n",
    "      1) spaCy PERSON anywhere in text → accept immediately (light filter)\n",
    "      2) If none and GUEST_HINTS present → parallel LLM extraction\n",
    "      3) If still none and title has \"feat./with/guest\" → spaCy PERSON from title\n",
    "    \"\"\"\n",
    "    titles = titles or [\"\"] * len(texts)\n",
    "    cleaned = [clean_comment(t) for t in texts]\n",
    "    results = [[] for _ in cleaned]\n",
    "\n",
    "    # 1) spaCy PERSON anywhere (batched)\n",
    "    docs = list(nlp.pipe(cleaned, batch_size=256, n_process=1))\n",
    "    for i, doc in enumerate(docs):\n",
    "        cand = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "        if cand:\n",
    "            cand = [n for n in cand if len(n) > 2 and n.lower() not in BANNED_GUEST_WORDS]\n",
    "            if cand:\n",
    "                results[i] = list(dict.fromkeys(cand))  # keep order\n",
    "\n",
    "    # 2) LLM for rows with hints but still empty\n",
    "    llm_idxs = [i for i, (t, r) in enumerate(zip(cleaned, results)) if not r and GUEST_HINTS.search(t or \"\")]\n",
    "    if llm_idxs:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = {ex.submit(_llm_extract_guests_single, cleaned[i], llm_model): i for i in llm_idxs}\n",
    "            for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Guests LLM (parallel)\"):\n",
    "                i = futs[fut]\n",
    "                try:\n",
    "                    results[i] = fut.result()\n",
    "                except Exception:\n",
    "                    results[i] = []\n",
    "\n",
    "    # 3) Title fallback\n",
    "    fallback_idxs = [i for i in range(len(cleaned)) if not results[i] and TITLE_CUE_RE.search(titles[i] or \"\")]\n",
    "    if fallback_idxs:\n",
    "        title_docs = list(nlp.pipe([titles[i] for i in fallback_idxs], batch_size=256, n_process=1))\n",
    "        for i, doc in zip(fallback_idxs, title_docs):\n",
    "            cand = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "            cand = [n for n in cand if len(n) > 2 and n.lower() not in BANNED_GUEST_WORDS]\n",
    "            results[i] = list(dict.fromkeys(cand))\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Topic Request Extraction — SAME LOGIC as Code 1, but parallel\n",
    "# -----------------------------\n",
    "def _llm_topic_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are analyzing a YouTube comment on a podcast episode.\n",
    "\n",
    "Your goal is to extract *specific topics or subjects the viewer explicitly wants discussed in future episodes.*\n",
    "\n",
    "Focus on:\n",
    "- Requests or suggestions like “talk about X”, “do an episode on Y”, “discuss more about Z”, “I’d love a conversation on…”.\n",
    "- Extract concise, meaningful topics (1–4 words) representing what the viewer wants to hear more about.\n",
    "- Ignore vague mentions, compliments, or generic categories (like “science”, “health”, “AI”) unless clearly framed as requests.\n",
    "- Exclude random nouns or things already being discussed.\n",
    "\n",
    "Return strictly a JSON list of short topic phrases (no explanation, no duplicates).\n",
    "\n",
    "Example:\n",
    "[\"mental health in men\", \"AI and creativity\", \"nutrition and longevity\"]\n",
    "\n",
    "Comment: \"{text}\"\n",
    "\"\"\".strip()\n",
    "\n",
    "def _llm_extract_topics_single(text: str, model_name: str = \"gemma:2b\") -> List[str]:\n",
    "    try:\n",
    "        resp = ollama.chat(model=model_name, messages=[{\"role\":\"user\",\"content\":_llm_topic_prompt(text)}])\n",
    "        topics = json.loads(resp[\"message\"][\"content\"])\n",
    "        if isinstance(topics, list):\n",
    "            topics = [str(t).strip().lower() for t in topics if str(t).strip()]\n",
    "            topics = list(dict.fromkeys(topics))[:5]\n",
    "            return topics\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def extract_topics_batch(texts: List[str], max_workers: int = 8, llm_model: str = \"gemma:2b\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Code 1 flow, compute-friendly:\n",
    "      • Only rows that match TOPIC_HINTS call the LLM (as in Code 1).\n",
    "      • Parallelize the LLM calls.\n",
    "    \"\"\"\n",
    "    cleaned = [clean_comment(t) for t in texts]\n",
    "    results = [[] for _ in cleaned]\n",
    "    idxs = [i for i, t in enumerate(cleaned) if t and TOPIC_HINTS.search(t)]\n",
    "    if not idxs:\n",
    "        return results\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(_llm_extract_topics_single, cleaned[i], llm_model): i for i in idxs}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Topics LLM (parallel)\"):\n",
    "            i = futs[fut]\n",
    "            try:\n",
    "                results[i] = fut.result()\n",
    "            except Exception:\n",
    "                results[i] = []\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Apply All to DataFrame — with caching & batching\n",
    "# -----------------------------\n",
    "def apply_sentiment_fast(df: pd.DataFrame,\n",
    "                         text_col: str = \"comment_text\",\n",
    "                         id_col: str = \"comment_id\",\n",
    "                         like_col: str = \"comment_like_count\",\n",
    "                         title_col: Optional[str] = \"video_title\",\n",
    "                         use_cache: bool = True,\n",
    "                         llm_workers: int = 8,\n",
    "                         llm_model: str = \"gemma:2b\",\n",
    "                         run_guest_topic: bool = True) -> pd.DataFrame:\n",
    "\n",
    "    assert text_col in df.columns, f\"'{text_col}' not in df\"\n",
    "    assert id_col in df.columns, f\"'{id_col}' not in df\"\n",
    "\n",
    "    _ensure_cache()\n",
    "    out = df.copy()\n",
    "    out[text_col] = out[text_col].fillna(\"\").astype(str)\n",
    "    out[\"cleaned_text\"] = out[text_col].map(clean_comment)\n",
    "\n",
    "    # sentiment cache prep\n",
    "    out[\"__text_hash\"] = out[\"cleaned_text\"].map(sha1)\n",
    "    ids = out[id_col].astype(str).tolist()\n",
    "    id_to_hash = dict(zip(ids, out[\"__text_hash\"].tolist()))\n",
    "    cached_sent = cache_get_sentiments(ids, id_to_hash) if use_cache else {}\n",
    "    cached_mask_sent = out[id_col].astype(str).isin(cached_sent.keys())\n",
    "\n",
    "    # init cols\n",
    "    for col in [\"sentiment_p_pos\", \"sentiment_bucket\", \"p_pos_llm\", \"p_pos_hf\", \"p_neg_hf\"]:\n",
    "        out[col] = np.nan if col != \"sentiment_bucket\" else None\n",
    "\n",
    "    # fill from cache\n",
    "    if cached_sent:\n",
    "        idx = out.index[cached_mask_sent]\n",
    "        for i in idx:\n",
    "            cid = str(out.at[i, id_col]); row = cached_sent[cid]\n",
    "            out.at[i, \"sentiment_p_pos\"] = row[\"p_pos\"]\n",
    "            out.at[i, \"sentiment_bucket\"] = row[\"bucket\"]\n",
    "            out.at[i, \"p_pos_llm\"] = row[\"p_pos_llm\"]\n",
    "            out.at[i, \"p_pos_hf\"] = row[\"p_pos_hf\"]\n",
    "            out.at[i, \"p_neg_hf\"] = row[\"p_neg_hf\"]\n",
    "\n",
    "    # compute missing (batched HF + parallel LLM, SAME LOGIC)\n",
    "    need = out.index[~cached_mask_sent].tolist()\n",
    "    if need:\n",
    "        texts = out.loc[need, \"cleaned_text\"].tolist()\n",
    "        scores = ensemble_sentiment_batch(texts, llm_workers=llm_workers, llm_model=llm_model)\n",
    "        out.loc[need, \"p_pos_hf\"] = scores[\"p_pos_hf\"]\n",
    "        out.loc[need, \"p_neg_hf\"] = scores[\"p_neg_hf\"]\n",
    "        out.loc[need, \"p_pos_llm\"] = scores[\"p_pos_llm\"]\n",
    "        out.loc[need, \"sentiment_p_pos\"] = scores[\"p_pos\"]\n",
    "        out.loc[need, \"sentiment_bucket\"] = scores[\"bucket\"]\n",
    "\n",
    "        if use_cache:\n",
    "            to_cache = []\n",
    "            for i in need:\n",
    "                to_cache.append((\n",
    "                    str(out.at[i, id_col]),\n",
    "                    out.at[i, \"__text_hash\"],\n",
    "                    float(out.at[i, \"sentiment_p_pos\"]),\n",
    "                    str(out.at[i, \"sentiment_bucket\"]),\n",
    "                    float(out.at[i, \"p_pos_llm\"]),\n",
    "                    float(out.at[i, \"p_pos_hf\"]),\n",
    "                    float(out.at[i, \"p_neg_hf\"]),\n",
    "                ))\n",
    "            cache_put_sentiments(to_cache)\n",
    "\n",
    "    # impact weighting\n",
    "    if like_col in out.columns:\n",
    "        out[like_col] = out[like_col].fillna(0).astype(int)\n",
    "        out[\"comment_weight\"] = out[like_col].map(like_weight)\n",
    "    else:\n",
    "        out[\"comment_weight\"] = 1.0\n",
    "    out[\"impact_weighted_sentiment\"] = out[\"sentiment_p_pos\"].astype(float) * out[\"comment_weight\"].astype(float)\n",
    "\n",
    "    # guests/topics (same logic, parallelized + cached)\n",
    "    if run_guest_topic:\n",
    "        cached_meta = cache_get_meta(ids, id_to_hash) if use_cache else {}\n",
    "        cached_mask_meta = out[id_col].astype(str).isin(cached_meta.keys())\n",
    "        out[\"guest_mentions\"] = [[] for _ in range(len(out))]\n",
    "        out[\"topic_requests\"] = [[] for _ in range(len(out))]\n",
    "\n",
    "        # from cache\n",
    "        if cached_meta:\n",
    "            idx = out.index[cached_mask_meta]\n",
    "            for i in idx:\n",
    "                cid = str(out.at[i, id_col]); row = cached_meta[cid]\n",
    "                out.at[i, \"guest_mentions\"] = row[\"guest_mentions\"]\n",
    "                out.at[i, \"topic_requests\"] = row[\"topic_requests\"]\n",
    "\n",
    "        # compute missing\n",
    "        need_meta = out.index[~cached_mask_meta].tolist()\n",
    "        if need_meta:\n",
    "            texts = out.loc[need_meta, \"cleaned_text\"].tolist()\n",
    "            titles = out[title_col].tolist() if title_col and title_col in out.columns else [\"\"] * len(out)\n",
    "\n",
    "            guests_lists = extract_guests_batch(texts, [titles[i] for i in need_meta],\n",
    "                                                max_workers=llm_workers, llm_model=llm_model)\n",
    "            topics_lists = extract_topics_batch(texts, max_workers=llm_workers, llm_model=llm_model)\n",
    "\n",
    "            for i, glst in zip(need_meta, guests_lists):\n",
    "                out.at[i, \"guest_mentions\"] = glst\n",
    "            for i, tlst in zip(need_meta, topics_lists):\n",
    "                out.at[i, \"topic_requests\"] = tlst\n",
    "\n",
    "            if use_cache:\n",
    "                to_meta = []\n",
    "                for i in need_meta:\n",
    "                    to_meta.append((\n",
    "                        str(out.at[i, id_col]),\n",
    "                        out.at[i, \"__text_hash\"],\n",
    "                        out.at[i, \"guest_mentions\"],\n",
    "                        out.at[i, \"topic_requests\"],\n",
    "                    ))\n",
    "                cache_put_meta(to_meta)\n",
    "\n",
    "    out.drop(columns=[\"__text_hash\"], inplace=True, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "# ===========================\n",
    "# Example\n",
    "# ===========================\n",
    "# out = apply_sentiment_fast(\n",
    "#     df,\n",
    "#     text_col=\"comment_text\",\n",
    "#     id_col=\"comment_id\",\n",
    "#     like_col=\"comment_like_count\",\n",
    "#     title_col=\"video_title\",\n",
    "#     use_cache=True,          # set False if you want fresh runs every time\n",
    "#     llm_workers=8,           # increase if you have more CPU cores / IO\n",
    "#     llm_model=\"gemma:2b\",\n",
    "#     run_guest_topic=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7070e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM sentiment (parallel):   0%|          | 0/96 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LLM sentiment (parallel): 100%|██████████| 96/96 [00:23<00:00,  4.17it/s]\n",
      "Guests LLM (parallel): 100%|██████████| 11/11 [00:04<00:00,  2.44it/s]\n",
      "Topics LLM (parallel): 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df = apply_sentiment_fast(\n",
    "    df,\n",
    "    text_col=\"comment_text\",\n",
    "    id_col=\"comment_id\",\n",
    "    like_col=\"comment_like_count\",\n",
    "    title_col=\"video_title\",\n",
    "    use_cache=True,          # set False if you want fresh runs every time\n",
    "    llm_workers=8,           # increase if you have more CPU cores / IO\n",
    "    llm_model=\"gemma:2b\",\n",
    "    run_guest_topic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "109a78e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>guest_list</th>\n",
       "      <th>Topic_Category</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment_p_pos</th>\n",
       "      <th>sentiment_bucket</th>\n",
       "      <th>p_pos_llm</th>\n",
       "      <th>p_pos_hf</th>\n",
       "      <th>p_neg_hf</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>comment_weight</th>\n",
       "      <th>impact_weighted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip Your Fat Faster Than Anything!</td>\n",
       "      <td>[Dr Benjamin Bikman]</td>\n",
       "      <td>health</td>\n",
       "      <td>Great to hear about your turn around.   Bravo!</td>\n",
       "      <td>0.926439</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.987398</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely &amp; Sexless Society!</td>\n",
       "      <td>[Dr Jordan Peterson]</td>\n",
       "      <td>mental health / psychology</td>\n",
       "      <td>Not a very open minded view....</td>\n",
       "      <td>0.110327</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.668515</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.132392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.</td>\n",
       "      <td>[Dr. Sara Szal]</td>\n",
       "      <td>health</td>\n",
       "      <td>In many countries</td>\n",
       "      <td>0.336931</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.061551</td>\n",
       "      <td>0.082644</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip Your Fat Faster Than Anything!</td>\n",
       "      <td>[Dr Benjamin Bikman]</td>\n",
       "      <td>health</td>\n",
       "      <td>1 hour and 43 minutes. He refferences biggest loser and he says they gain it all back. Which is ...</td>\n",
       "      <td>0.145619</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.802823</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.145619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip Your Fat Faster Than Anything!</td>\n",
       "      <td>[Dr Benjamin Bikman]</td>\n",
       "      <td>health</td>\n",
       "      <td>This channel needs to catch up with the world and what matters the most!</td>\n",
       "      <td>0.308528</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.180879</td>\n",
       "      <td>0.126774</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.370233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!</td>\n",
       "      <td>[Master Shi Heng Yi]</td>\n",
       "      <td>other</td>\n",
       "      <td>I'm happy for you</td>\n",
       "      <td>0.920455</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.980759</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Peter Attia: Anti-aging Cure No One Talks About! 50% Chance You’ll Die In A Year If This Happens!</td>\n",
       "      <td>[Dr Peter Attia]</td>\n",
       "      <td>health</td>\n",
       "      <td>Peter said 15% die within a year - not 50%.  Click bait?</td>\n",
       "      <td>0.105774</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.105774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip Your Fat Faster Than Anything!</td>\n",
       "      <td>[Dr Benjamin Bikman]</td>\n",
       "      <td>health</td>\n",
       "      <td>Is that for us to answer? If you had to ask and it's for you, I guess you know the answer</td>\n",
       "      <td>0.346968</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.078279</td>\n",
       "      <td>0.097291</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Business Expert: How To Build A Brand In 2025! They're Lying To You About Work-Life Balance!</td>\n",
       "      <td>[Emma Grede]</td>\n",
       "      <td>productivity / personal development</td>\n",
       "      <td>Literally proves her point about an employee mindset....</td>\n",
       "      <td>0.421810</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.146351</td>\n",
       "      <td>0.099479</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.506172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Insulin &amp; Glucose Doctor: This Will Strip Your Fat Faster Than Anything!</td>\n",
       "      <td>[Dr Benjamin Bikman]</td>\n",
       "      <td>health</td>\n",
       "      <td>the brain needs cholesterol. \\ngreat guest. ty</td>\n",
       "      <td>0.674951</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.674918</td>\n",
       "      <td>0.050141</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.674951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          video_title  \\\n",
       "0                        The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!   \n",
       "1         Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!   \n",
       "2    Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.   \n",
       "3                        The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!   \n",
       "4                        The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!   \n",
       "5  Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!   \n",
       "6   Peter Attia: Anti-aging Cure No One Talks About! 50% Chance You’ll Die In A Year If This Happens!   \n",
       "7                        The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!   \n",
       "8    The Business Expert: How To Build A Brand In 2025! They're Lying To You About Work-Life Balance!   \n",
       "9                        The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!   \n",
       "\n",
       "             guest_list                       Topic_Category  \\\n",
       "0  [Dr Benjamin Bikman]                               health   \n",
       "1  [Dr Jordan Peterson]           mental health / psychology   \n",
       "2       [Dr. Sara Szal]                               health   \n",
       "3  [Dr Benjamin Bikman]                               health   \n",
       "4  [Dr Benjamin Bikman]                               health   \n",
       "5  [Master Shi Heng Yi]                                other   \n",
       "6      [Dr Peter Attia]                               health   \n",
       "7  [Dr Benjamin Bikman]                               health   \n",
       "8          [Emma Grede]  productivity / personal development   \n",
       "9  [Dr Benjamin Bikman]                               health   \n",
       "\n",
       "                                                                                          cleaned_text  \\\n",
       "0                                                       Great to hear about your turn around.   Bravo!   \n",
       "1                                                                      Not a very open minded view....   \n",
       "2                                                                                    In many countries   \n",
       "3  1 hour and 43 minutes. He refferences biggest loser and he says they gain it all back. Which is ...   \n",
       "4                             This channel needs to catch up with the world and what matters the most!   \n",
       "5                                                                                    I'm happy for you   \n",
       "6                                             Peter said 15% die within a year - not 50%.  Click bait?   \n",
       "7            Is that for us to answer? If you had to ask and it's for you, I guess you know the answer   \n",
       "8                                             Literally proves her point about an employee mindset....   \n",
       "9                                                       the brain needs cholesterol. \\ngreat guest. ty   \n",
       "\n",
       "   sentiment_p_pos sentiment_bucket  p_pos_llm  p_pos_hf  p_neg_hf  \\\n",
       "0         0.926439         Positive      0.835  0.987398  0.003098   \n",
       "1         0.110327         Negative      0.750  0.017211  0.668515   \n",
       "2         0.336931         Negative      0.750  0.061551  0.082644   \n",
       "3         0.145619         Negative      0.850  0.009365  0.802823   \n",
       "4         0.308528         Negative      0.500  0.180879  0.126774   \n",
       "5         0.920455         Positive      0.830  0.980759  0.003789   \n",
       "6         0.105774         Negative      0.750  0.009624  0.682345   \n",
       "7         0.346968         Negative      0.750  0.078279  0.097291   \n",
       "8         0.421810          Neutral      0.835  0.146351  0.099479   \n",
       "9         0.674951         Positive      0.675  0.674918  0.050141   \n",
       "\n",
       "   comment_like_count  comment_weight  impact_weighted_sentiment  \n",
       "0                   0             1.0                   0.926439  \n",
       "1                   1             1.2                   0.132392  \n",
       "2                   0             1.0                   0.336931  \n",
       "3                   0             1.0                   0.145619  \n",
       "4                   1             1.2                   0.370233  \n",
       "5                   0             1.0                   0.920455  \n",
       "6                   0             1.0                   0.105774  \n",
       "7                   0             1.0                   0.346968  \n",
       "8                   1             1.2                   0.506172  \n",
       "9                   0             1.0                   0.674951  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['video_title', 'guest_list','Topic_Category','cleaned_text', 'sentiment_p_pos', 'sentiment_bucket', 'p_pos_llm', 'p_pos_hf', 'p_neg_hf', 'comment_like_count', 'comment_weight', 'impact_weighted_sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4718d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_bucket\n",
      "Negative    56\n",
      "Positive    29\n",
      "Neutral     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "comment_weight\n",
      "1.0    69\n",
      "1.2    28\n",
      "2.0     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "guest_mentions\n",
      "[]                                                  82\n",
      "[Ben Bikman]                                         2\n",
      "[Doc]                                                1\n",
      "[Steven]                                             1\n",
      "[Keto]                                               1\n",
      "[Steven, Tim Spector - a]                            1\n",
      "[Jordan Peterson, Dr. Rhonda Patrick, Elon Musk]     1\n",
      "[Dr. Rhonda Patrick, Elon Musk]                      1\n",
      "[Jordan Peterson]                                    1\n",
      "[Benjamin Bikman]                                    1\n",
      "[Jillian michaels]                                   1\n",
      "[Bernstein]                                          1\n",
      "[Joe Rogan]                                          1\n",
      "[Bendiciones]                                        1\n",
      "[Jordan B Peterson, Steven]                          1\n",
      "[name]                                               1\n",
      "[Peter]                                              1\n",
      "[Dr Peterson]                                        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "topic_requests\n",
      "[]                                                                                                                   97\n",
      "[topics]                                                                                                              2\n",
      "[{'topic': 'recovery from interview'}, {'topic': 'work-life balance'}, {'topic': 'sacrifice of self for career'}]     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment_bucket'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(df['comment_weight'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(df['guest_mentions'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(df['topic_requests'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02c6c782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_title                                                                                         sentiment_bucket\n",
       "Body Language Expert Explains Why People Dislike You                                                Negative            16\n",
       "                                                                                                    Positive             5\n",
       "                                                                                                    Neutral              1\n",
       "Exercise & Nutrition Scientist: The Truth About Exercise On Your Period! Take These 4 Supplements!  Negative             3\n",
       "                                                                                                    Neutral              3\n",
       "                                                                                                    Positive             1\n",
       "Hormone Expert: Control Your Hormones Control Your Belly Fat! Cortisol, oestrogen, testosterone.    Negative             2\n",
       "Jordan B Peterson: You Need To Listen To Your Wife! We've Built A Lonely & Sexless Society!         Negative             7\n",
       "                                                                                                    Neutral              4\n",
       "                                                                                                    Positive             4\n",
       "Peter Attia: Anti-aging Cure No One Talks About! 50% Chance You’ll Die In A Year If This Happens!   Negative             3\n",
       "                                                                                                    Positive             3\n",
       "Shaolin Warrior Master: Hidden Epidemic Nobody Talks About! This Modern Habit Is Killing Millions!  Neutral              4\n",
       "                                                                                                    Positive             3\n",
       "The Business Expert: How To Build A Brand In 2025! They're Lying To You About Work-Life Balance!    Negative             2\n",
       "                                                                                                    Neutral              1\n",
       "The Insulin & Glucose Doctor: This Will Strip Your Fat Faster Than Anything!                        Negative            23\n",
       "                                                                                                    Positive            13\n",
       "                                                                                                    Neutral              2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('video_title')[['sentiment_bucket']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f28e39",
   "metadata": {},
   "source": [
    "# What's missing or could be improved?\n",
    "\n",
    "- How to best score sentiment against the topic or guests or both and give them weights that make sense\n",
    "- an LLM that goes over the guest_mentions and topic_requests to clean and normalize them (GPT4 should do)\n",
    "- Visualise results in a way that is interesting and helps make informed decisions about future guests/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bde0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-diary-of-a-ceo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
