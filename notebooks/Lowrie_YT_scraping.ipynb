{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "141a038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69d7e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER SETTINGS\n",
    "# it searches for videos whose title, description, or tags contain that keyword — not the channel name.\n",
    "KEYWORDS = [    \"podcast\", \"interview\", \"talk\", \"conversation\", \n",
    "    \"discussion\", \"life podcast\", \"tech podcast\", \"data podcast\"]\n",
    "\n",
    "SUB_MIN = 400       # minimum subscribers\n",
    "SUB_MAX = 3000      # maximum subscribers\n",
    "MIN_VIDEOS = 15          # minimum total uploads on the channel\n",
    "MAX_VIDEOS = 60          # maximum total uploads on the channel\n",
    "\n",
    "RECENT_DAYS_FOR_ACTIVITY = 15   # must have uploaded within last 10 days\n",
    "DAYS_FOR_FREQ = 30             # look at uploads in past 30 days\n",
    "MIN_UPLOADS_IN_30_DAYS = 2     # roughly 1 video/week\n",
    "\n",
    "# === Quota-saving options ===\n",
    "MAX_CHANNELS = 100        # analyze max 100 channels per run\n",
    "MAX_SEARCH_PAGES = 5      # fetch only 5 pages of search results (≈250 videos)\n",
    "CHECK_VIDEO_DURATION = False  # set True to use videos.list for exact duration, False to skip (uses only #shorts filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b490a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso8601(dt):\n",
    "    \"\"\"Return datetime in YouTube API compatible UTC format (YYYY-MM-DDTHH:MM:SSZ).\"\"\"\n",
    "    # ensure it's UTC and strip microseconds + timezone offset\n",
    "    return dt.replace(microsecond=0, tzinfo=None).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "\n",
    "\n",
    "def search_recent_videos_for_keyword(keyword, published_after_iso, max_pages=MAX_SEARCH_PAGES):\n",
    "    \"\"\"Search recent videos by keyword and return a set of channel IDs that uploaded them.\"\"\"\n",
    "    channel_ids = set()\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"q\": keyword,\n",
    "        \"type\": \"video\",\n",
    "        \"order\": \"date\",\n",
    "        \"publishedAfter\": published_after_iso,\n",
    "        \"maxResults\": 50,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    nextPageToken = None\n",
    "    page = 0\n",
    "    while page < max_pages:\n",
    "        if nextPageToken:\n",
    "            params[\"pageToken\"] = nextPageToken\n",
    "\n",
    "        try:\n",
    "            r = requests.get(\"https://www.googleapis.com/youtube/v3/search\", params=params)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"⚠️ API error while searching '{keyword}': {e}\")\n",
    "            break\n",
    "\n",
    "        data = r.json()\n",
    "        for item in data.get(\"items\", []):\n",
    "            channel_ids.add(item[\"snippet\"][\"channelId\"])\n",
    "\n",
    "        nextPageToken = data.get(\"nextPageToken\")\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    return channel_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1181ac",
   "metadata": {},
   "source": [
    "- This step gives us the pool of candidates.\n",
    "- They’re all recent, relevant, and active in your niche — the next step is to check which ones fit your target (subscriber count, total videos, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4f1ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_info(channel_ids):\n",
    "    \"\"\"\n",
    "    Get subscriber count, total videos, handle, and basic info for each channel.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    ids = list(channel_ids)\n",
    "\n",
    "    for i in range(0, len(ids), 50):  # can fetch up to 50 channels per API call\n",
    "        chunk = ids[i:i + 50]\n",
    "        params = {\n",
    "            \"part\": \"snippet,statistics,contentDetails\",\n",
    "            \"id\": \",\".join(chunk),\n",
    "            \"maxResults\": 50,\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        r = requests.get(\"https://www.googleapis.com/youtube/v3/channels\", params=params)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        for ch in data.get(\"items\", []):\n",
    "            cid = ch[\"id\"]\n",
    "            snippet = ch.get(\"snippet\", {})\n",
    "            stats = ch.get(\"statistics\", {})\n",
    "            content = ch.get(\"contentDetails\", {})\n",
    "\n",
    "            # customUrl often returns '@handle' (or legacy custom URL). Use if present.\n",
    "            handle = snippet.get(\"customUrl\") or None\n",
    "\n",
    "            result[cid] = {\n",
    "                \"title\": snippet.get(\"title\"),\n",
    "                \"description\": snippet.get(\"description\"),\n",
    "                \"publishedAt\": snippet.get(\"publishedAt\"),\n",
    "                \"subscriberCount\": int(stats.get(\"subscriberCount\") or 0),\n",
    "                \"videoCount\": int(stats.get(\"videoCount\") or 0),\n",
    "                \"uploadsPlaylistId\": content.get(\"relatedPlaylists\", {}).get(\"uploads\"),\n",
    "                \"handle\": handle\n",
    "            }\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890454a",
   "metadata": {},
   "source": [
    "what we got now are details for each of those channels — like subscriber count, video count, and when they started — so we can filter by the chosen range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0e196ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_to_seconds(iso_duration: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert 'PT#H#M#S' to seconds. Returns 0 if unknown.\n",
    "    \"\"\"\n",
    "    if not iso_duration:\n",
    "        return 0\n",
    "    m = re.fullmatch(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", iso_duration)\n",
    "    if not m:\n",
    "        return 0\n",
    "    h = int(m.group(1) or 0)\n",
    "    mm = int(m.group(2) or 0)\n",
    "    s = int(m.group(3) or 0)\n",
    "    return h * 3600 + mm * 60 + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9fa821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_recent_longform_uploads(channel_id, published_after_iso, max_pages=3, min_seconds=120):\n",
    "    \"\"\"\n",
    "    Count long-form uploads after the given date.\n",
    "    Uses #shorts filter only if CHECK_VIDEO_DURATION=False to save quota.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"channelId\": channel_id,\n",
    "        \"type\": \"video\",\n",
    "        \"order\": \"date\",\n",
    "        \"publishedAfter\": published_after_iso,\n",
    "        \"maxResults\": 50,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    nextPageToken = None\n",
    "    page = 0\n",
    "    while page < max_pages:\n",
    "        if nextPageToken:\n",
    "            params[\"pageToken\"] = nextPageToken\n",
    "\n",
    "        r = requests.get(\"https://www.googleapis.com/youtube/v3/search\", params=params)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"⚠️ Skipping channel {channel_id} (quota or request error)\")\n",
    "            break\n",
    "        data = r.json()\n",
    "        items = data.get(\"items\", [])\n",
    "\n",
    "        # collect IDs excluding Shorts by title\n",
    "        ids = []\n",
    "        for it in items:\n",
    "            title = (it[\"snippet\"][\"title\"] or \"\").lower()\n",
    "            if \"#shorts\" in title:\n",
    "                continue\n",
    "            vid = it.get(\"id\", {}).get(\"videoId\")\n",
    "            if vid:\n",
    "                ids.append(vid)\n",
    "\n",
    "        if CHECK_VIDEO_DURATION:\n",
    "            # heavier version: check duration\n",
    "            for i in range(0, len(ids), 50):\n",
    "                chunk = ids[i:i+50]\n",
    "                vr = requests.get(\n",
    "                    \"https://www.googleapis.com/youtube/v3/videos\",\n",
    "                    params={\"part\": \"contentDetails\", \"id\": \",\".join(chunk), \"key\": API_KEY}\n",
    "                )\n",
    "                if vr.status_code != 200:\n",
    "                    continue\n",
    "                vdata = vr.json()\n",
    "                for v in vdata.get(\"items\", []):\n",
    "                    dur = v.get(\"contentDetails\", {}).get(\"duration\")\n",
    "                    if duration_to_seconds(dur) >= min_seconds:\n",
    "                        total += 1\n",
    "        else:\n",
    "            # light version: just count non-shorts\n",
    "            total += len(ids)\n",
    "\n",
    "        nextPageToken = data.get(\"nextPageToken\")\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a87fc",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fe4f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_channels(channels_info, category_label=\"podcast\"):\n",
    "    \"\"\"\n",
    "    Apply filters and return list of dicts with these keys:\n",
    "    Youtuber, handle, category, subs, lifetime_uploads, latest_upload\n",
    "    \"\"\"\n",
    "    print(\"\\n🛠️ Filtering channels based on activity and size...\")\n",
    "\n",
    "    published_after_freq = iso8601(datetime.now(timezone.utc) - timedelta(days=DAYS_FOR_FREQ))\n",
    "    published_after_activity = iso8601(datetime.now(timezone.utc) - timedelta(days=RECENT_DAYS_FOR_ACTIVITY))\n",
    "    rows = []\n",
    "\n",
    "    for cid, info in list(channels_info.items())[:MAX_CHANNELS]:\n",
    "        subs = info[\"subscriberCount\"]\n",
    "        vids = info[\"videoCount\"]\n",
    "\n",
    "        if (subs < SUB_MIN or subs > SUB_MAX) or (vids < MIN_VIDEOS or vids > MAX_VIDEOS):\n",
    "            continue\n",
    "\n",
    "\n",
    "        # long-form uploads in last 30 days\n",
    "        uploads_30 = count_recent_longform_uploads(cid, published_after_freq)\n",
    "\n",
    "        # latest upload date (any video, but you can also force long-form with a heavier call)\n",
    "        r = requests.get(\n",
    "            \"https://www.googleapis.com/youtube/v3/search\",\n",
    "            params={\n",
    "                \"part\": \"snippet\",\n",
    "                \"channelId\": cid,\n",
    "                \"type\": \"video\",\n",
    "                \"order\": \"date\",\n",
    "                \"maxResults\": 1,\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "        )\n",
    "        latest_items = r.json().get(\"items\", [])\n",
    "        latest_date = latest_items[0][\"snippet\"][\"publishedAt\"] if latest_items else None\n",
    "\n",
    "        # active in last X days?\n",
    "        active_ok = False\n",
    "        if latest_date:\n",
    "            dt_latest = datetime.fromisoformat(latest_date.replace(\"Z\", \"+00:00\"))\n",
    "            active_ok = (datetime.now(timezone.utc) - dt_latest).days <= RECENT_DAYS_FOR_ACTIVITY\n",
    "\n",
    "        if active_ok and uploads_30 >= MIN_UPLOADS_IN_30_DAYS:\n",
    "            rows.append({\n",
    "                \"Youtuber\": info[\"title\"],\n",
    "                \"handle\": info.get(\"handle\"),\n",
    "                \"category\": category_label,\n",
    "                \"subs\": subs,\n",
    "                \"lifetime_uploads\": vids,\n",
    "                \"latest_upload\": latest_date,\n",
    "                # keep internal fields if you still want them later:\n",
    "                \"_uploads_last30_longform\": uploads_30,\n",
    "                \"_channelId\": cid,\n",
    "                \"_channelUrl\": f\"https://www.youtube.com/channel/{cid}\"\n",
    "            })\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"✅ Found {len(rows)} matching channels.\")\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2981d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(rows):\n",
    "    \"\"\"\n",
    "    Turn results into a clean DataFrame with the requested columns.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"Youtuber\", \"handle\", \"category\", \"subs\", \"lifetime_uploads\", \"latest_upload\"])\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Only keep the public columns (you can drop the underscored ones)\n",
    "    return df[[\"Youtuber\", \"handle\", \"category\", \"subs\", \"lifetime_uploads\", \"latest_upload\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06e52f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Searching YouTube for 8 keywords...\n",
      "\n",
      "🔍 Searching for keyword: podcast\n",
      "  ➜ Found 216 channels for 'podcast'\n",
      "\n",
      "🔍 Searching for keyword: interview\n",
      "  ➜ Found 194 channels for 'interview'\n",
      "\n",
      "🔍 Searching for keyword: talk\n",
      "  ➜ Found 180 channels for 'talk'\n",
      "\n",
      "🔍 Searching for keyword: conversation\n",
      "  ➜ Found 206 channels for 'conversation'\n",
      "\n",
      "🔍 Searching for keyword: discussion\n",
      "  ➜ Found 201 channels for 'discussion'\n",
      "\n",
      "🔍 Searching for keyword: life podcast\n",
      "  ➜ Found 212 channels for 'life podcast'\n",
      "\n",
      "🔍 Searching for keyword: tech podcast\n",
      "  ➜ Found 177 channels for 'tech podcast'\n",
      "\n",
      "🔍 Searching for keyword: data podcast\n",
      "  ➜ Found 207 channels for 'data podcast'\n",
      "\n",
      "📊 Total unique channels found: 1537\n",
      "📦 Loading cached channel info...\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "✅ Found 2 matching channels.\n",
      "✅ 2 channels matched after filtering for 'podcast'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "✅ Found 2 matching channels.\n",
      "✅ 2 channels matched after filtering for 'interview'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "✅ Found 2 matching channels.\n",
      "✅ 2 channels matched after filtering for 'talk'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "⚠️ Skipping channel UCYbgVAWWqniZQvBLpePxtcA (quota or request error)\n",
      "✅ Found 1 matching channels.\n",
      "✅ 1 channels matched after filtering for 'conversation'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "⚠️ Skipping channel UCk-IwV8W8gjr5dwHKz9s_LA (quota or request error)\n",
      "⚠️ Skipping channel UCL45W2iEVh9EXtsISYrHqxQ (quota or request error)\n",
      "⚠️ Skipping channel UCYbgVAWWqniZQvBLpePxtcA (quota or request error)\n",
      "✅ Found 0 matching channels.\n",
      "✅ 0 channels matched after filtering for 'discussion'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "⚠️ Skipping channel UCLERAOnkv8voweuvafsLOsg (quota or request error)\n",
      "⚠️ Skipping channel UCk-IwV8W8gjr5dwHKz9s_LA (quota or request error)\n",
      "⚠️ Skipping channel UCL45W2iEVh9EXtsISYrHqxQ (quota or request error)\n",
      "⚠️ Skipping channel UCYbgVAWWqniZQvBLpePxtcA (quota or request error)\n",
      "✅ Found 0 matching channels.\n",
      "✅ 0 channels matched after filtering for 'life podcast'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "⚠️ Skipping channel UCLERAOnkv8voweuvafsLOsg (quota or request error)\n",
      "⚠️ Skipping channel UCk-IwV8W8gjr5dwHKz9s_LA (quota or request error)\n",
      "⚠️ Skipping channel UCL45W2iEVh9EXtsISYrHqxQ (quota or request error)\n",
      "⚠️ Skipping channel UCYbgVAWWqniZQvBLpePxtcA (quota or request error)\n",
      "✅ Found 0 matching channels.\n",
      "✅ 0 channels matched after filtering for 'tech podcast'\n",
      "\n",
      "🛠️ Filtering channels based on activity and size...\n",
      "⚠️ Skipping channel UCLERAOnkv8voweuvafsLOsg (quota or request error)\n",
      "⚠️ Skipping channel UCk-IwV8W8gjr5dwHKz9s_LA (quota or request error)\n",
      "⚠️ Skipping channel UCL45W2iEVh9EXtsISYrHqxQ (quota or request error)\n",
      "⚠️ Skipping channel UCYbgVAWWqniZQvBLpePxtcA (quota or request error)\n",
      "✅ Found 0 matching channels.\n",
      "✅ 0 channels matched after filtering for 'data podcast'\n",
      "\n",
      "🧾 Final DataFrame preview:\n",
      "         Youtuber               handle     category  subs  lifetime_uploads        latest_upload\n",
      "NIRSATY  निर्सत्य         @satynir7989      podcast   720                35 2025-10-09T07:12:16Z\n",
      " Jhakaas Junction @jhakaasjunction-n3j      podcast  2330                26 2025-10-09T05:06:31Z\n",
      "NIRSATY  निर्सत्य         @satynir7989    interview   720                35 2025-10-09T07:12:16Z\n",
      " Jhakaas Junction @jhakaasjunction-n3j    interview  2330                26 2025-10-09T05:06:31Z\n",
      "NIRSATY  निर्सत्य         @satynir7989         talk   720                35 2025-10-09T07:12:16Z\n",
      " Jhakaas Junction @jhakaasjunction-n3j         talk  2330                26 2025-10-09T05:06:31Z\n",
      "NIRSATY  निर्सत्य         @satynir7989 conversation   720                35 2025-10-09T07:12:16Z\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1️⃣ Find channels from all keywords\n",
    "    published_after = iso8601(datetime.now(timezone.utc) - timedelta(days=30))\n",
    "    all_channels = set()\n",
    "\n",
    "    print(f\"🎯 Searching YouTube for {len(KEYWORDS)} keywords...\")\n",
    "    for kw in KEYWORDS:\n",
    "        print(f\"\\n🔍 Searching for keyword: {kw}\")\n",
    "        found = search_recent_videos_for_keyword(kw, published_after)\n",
    "        print(f\"  ➜ Found {len(found)} channels for '{kw}'\")\n",
    "        all_channels |= found  # merge into global set\n",
    "\n",
    "    print(f\"\\n📊 Total unique channels found: {len(all_channels)}\")\n",
    "\n",
    "    # 2️⃣ Get channel info (with caching)\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "\n",
    "    cache_file = Path(\"channels_cache.json\")\n",
    "\n",
    "    if cache_file.exists():\n",
    "        print(\"📦 Loading cached channel info...\")\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            info = json.load(f)\n",
    "    else:\n",
    "        print(\"📡 Fetching new channel info...\")\n",
    "        info = get_channel_info(all_channels)\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 3️⃣ Filter and tag results by keyword category\n",
    "    combined_rows = []\n",
    "    for kw in KEYWORDS:\n",
    "        filtered = filter_channels(info, category_label=kw)\n",
    "        print(f\"✅ {len(filtered)} channels matched after filtering for '{kw}'\")\n",
    "        combined_rows.extend(filtered)\n",
    "\n",
    "    # 4️⃣ Combine into one DataFrame\n",
    "    df = to_dataframe(combined_rows)\n",
    "    print(\"\\n🧾 Final DataFrame preview:\")\n",
    "    print(df.head(20).to_string(index=False))\n",
    "\n",
    "    # 5️⃣ Save (optional)\n",
    "    #df.to_csv(\"found_channels.csv\", index=False)\n",
    "    #print(\"\\n💾 Saved all channels to found_channels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442341c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-diary-of-a-ceo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
